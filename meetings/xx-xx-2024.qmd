---
title: "Meeting --- Summary LLM"
author: "Bas Donkers, Dennis Fok, Finn-Ole HÃ¶ner"
date: last-modified
date-format: long
format: beamer
aspectratio: 169
incremental: false
include-in-header: ../beamer_preamble.tex
bibliography: ../references.bib
fig-align: center
fig-cap-location: bottom
fig-format: pdf
links-as-notes: true
number-depth: 1
eval: false
echo: true
fig-width: 12
fig-height: 8
---

# Updates

- Gradient spikes first, then large step in decoder / encoder
- These steps seem to affect one hidden unit more strongly than the other
- Increase of Decoder Weight gradients seems to precede the increase of the -LL
- The other gradients seem to react to this jump
- Gradient-Decoder $\to$ Step $\to$ Other gradients $\to$ Step ...
- [`torch.nn.utils.clip_grad_norm_(ae.parameters(), max_norm=dMaxGrad)`](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_) seems to help. This clips the gradient if the norm exceeds `dMaxGrad`. [See here](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf#page=6.78).

---

::: {#fig-elephants layout-ncol=2}

![Epoch 970-980](./figures/dashboard_last10_980.png)

![Epoch 980-990](./figures/dashboard_last10_990.png)

Spikes: Notice how the decoder gradient norm increases first, causing a large step and in-turn affecting the other gradients. The scales of the y-axis differ across all plots.
:::

---

![](./figures/dashboard_last10_540.png)

---

![](./figures/dashboard_last10_160.png)

---

![](./figures/dashboard_last10_350.png)

---

![Fixed clipping at 0.3. Later in the optimization this is too large.](./figures/dashboard_clipping_fixed.png)




---

Likely, you only want to clip true "outliers" in the gradients and not be too conservative. 

![You can overdo it with the gradient clipping.](./figures/dashboard_clip_01.png)




