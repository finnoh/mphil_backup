---
title: "Meeting --- Summary LLM"
author: "Bas Donkers, Dennis Fok, Finn-Ole HÃ¶ner"
date: 2024-05-07
date-format: long
format: beamer
aspectratio: 169
incremental: false
include-in-header: ../beamer_preamble.tex
bibliography: ../references.bib
fig-align: center
fig-cap-location: bottom
fig-format: pdf
links-as-notes: true
number-depth: 1
eval: false
echo: true
fig-width: 12
fig-height: 8
---

# Updates

- Possible source for the spikes: The way the likelihoods are generated by the LLM.
  - We always select the respective token for each position in the target sequence.
  - However, if we generate a wrong token earlier in the sequence, then we select the probability of this token, given the wrong token.
  - Instead, we should still select the "chain" of probabilities for the target sequence
  - A possible solution: Apply a logits processor, that forces the model to always generate the correct sequence. Summary embedding that we train this way, still generate the target sequence with an unconstrained model, but should be more smooth to train!


"Transition scores for each vocabulary token at each generation step. Beam transition scores consisting of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam. Tuple of torch.FloatTensor with up to max_new_tokens elements (one element for each generated token), with each tensor of shape (batch_size*num_beams, config.vocab_size)." ([link](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores))

`.compute_transition_scores()` very similar to our functions; Computes the conditional probabilities for the generated sequence. Taking the sum, yields the likelihood of the generated sequence.

Does calling this function with the target sequence, yield the correct target likelihood?

Use LLM judge to assess grammar?