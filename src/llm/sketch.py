# SETUP --------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.autograd as autograd
#from torchviz import make_dot
from transformers import AutoTokenizer, GPT2LMHeadModel

from undecorated import undecorated
from types import MethodType

# FUNCTIONS --------------------------
import torch

b_use_backend = True
i_seed_value = 42
s_model = 'gpt2'
#s_model = 'GroNLP/gpt2-small-dutch'

# INIT
if b_use_backend:
    if torch.backends.mps.is_available():
        mps_device = torch.device("mps")
        x = torch.ones(1, device=mps_device)
        torch.set_default_device('mps')
        print (f"Using MPS backend {x}")
    elif torch.cuda.is_available():
        print ("Using CUDA backend")
        torch.set_default_device('cuda')
    else:
        print ("Using CPU.")

#s_prompt_string = "A hot"
#s_target_string = "Potato"
# s_target_string = "easy example." # text generated by this prompt for this seed!
# s_target_string = "does it make sense to compare apples and oranges?" # text generated by this prompt for this seed!
s_target_string = "one, two," # text generated by this prompt for this seed!
# s_target_string = "Transform Your Hair's Health with our Revolutionary Haircare System: Experience Unparalleled Shine, Strength, and Vitality!" # text generated by this prompt for this seed!
# #s_target_string = "Heeft het zin om appels en sinaasappelen te vergelijken?" # text generated by this prompt for this seed!

torch.manual_seed(i_seed_value)
model = GPT2LMHeadModel.from_pretrained(s_model)
tokenizer = AutoTokenizer.from_pretrained(s_model)
tokenizer.pad_token_id = tokenizer.eos_token_id

# BUG: Issue that greedy search does not find max likelihood sequence? Does this matter?
i_num_beams = 1 # greedy search for i_num_beams = 1
i_no_repeat_ngram_size = 1 # TODO: This creates inplace gradient error
i_num_return = 1
i_max_new_tokens = tokenizer.encode(s_target_string, return_tensors="pt").shape[1]

inputs = tokenizer(s_target_string, return_tensors="pt")
embedding = model.get_input_embeddings()(inputs["input_ids"])
labels = inputs["input_ids"]
print(labels)

print(embedding.mean(dim=1).shape)
print(torch.tensor([[1115]]).shape)
print(inputs["input_ids"].shape)
out = model(inputs_embeds = embedding, labels = torch.tensor([[-100, -100, -100, 11]]))
print(out.loss)