# SETUP --------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.autograd as autograd
from torchviz import make_dot
from transformers import AutoTokenizer, GPT2LMHeadModel

from undecorated import undecorated
from types import MethodType

# FUNCTIONS --------------------------

def llikelihood(output: tuple, target_sequence: torch.Tensor) -> float:
    """Calculate the log-likelihood of the target tokens in the output sequence.
    NOTE: This only works for greedy search, not beam search (i.e. set num_beams=1).

    Args:
        output (tuple): Output from a HF LLM text generation model. Only pass one sequence.
        target_sequence (torch.Tensor): The target tokens (the sequence) to calculate the log-likelihood of.

    Returns:
        float: log-llikelihood for the sequence
    """
    
    # get scores and the sequences
    #scores = autograd.Variable(torch.stack(output.scores).reshape(len(output.scores), -1).transpose(0, 1), requires_grad=True)
    scores = torch.stack(output.scores).reshape(len(output.scores), -1).transpose(0, 1)
    
    # get the scores into a matrix, use log_softmax
    #m_prob = autograd.Variable(torch.nn.functional.log_softmax(scores, dim=0), requires_grad=True)
    m_prob = torch.nn.functional.log_softmax(scores, dim=0)
    #m_prob_sub = autograd.Variable(m_prob[target_sequence.squeeze()], requires_grad=True)
    m_prob_sub = m_prob[target_sequence.squeeze()]
    
    # get the diagonal of the matrix 
    #v_trans_prob = autograd.Variable(torch.diag(m_prob_sub), requires_grad=True)
    v_trans_prob = torch.diag(m_prob_sub)
    #d_llikelihood = autograd.Variable(torch.sum(v_trans_prob) * (-1), requires_grad=True)
    # NOTE: we use a minimizer
    d_llikelihood = torch.sum(v_trans_prob) * (-1)
    
    # return the log-likelihood of the target tokens and the diagonal
    #return m_prob, v_trans_prob, d_llikelihood
    return d_llikelihood


# TODO: Criterion

# MAGICKS --------------------------
i_seed_value = 42

s_model = 'gpt2'

s_prompt_string = "Finn writes code"
#s_target_string = "Potato"
s_target_string = "to make it easier to" # text generated by this prompt for this seed!

# BUG: Issue that greedy search does not find max likelihood sequence? Does this matter?
i_num_beams = 1 # greedy search for i_num_beams = 1
i_num_return = 1
i_new_tokens = 5
i_max_new_tokens = 5 # TODO: Set this automatically to the number tokens of the s_target_string after encoding

# INIT
torch.manual_seed(i_seed_value)
model = GPT2LMHeadModel.from_pretrained(s_model)
tokenizer = AutoTokenizer.from_pretrained(s_model)
tokenizer.pad_token_id = tokenizer.eos_token_id

# TODO: Link to this https://github.com/huggingface/transformers/issues/15552 (last comment)
generate_with_grad = model.generate.__closure__[1].cell_contents
model.generate_with_grad = MethodType(generate_with_grad, model)

input_embedding = autograd.Variable(torch.randn(1, 5, 768, requires_grad=True), requires_grad=True)
#input_embedding = autograd.Variable(model.get_input_embeddings()(tokenizer.encode(s_prompt_string, return_tensors="pt")), requires_grad=True)
target_sequence = tokenizer.encode(s_target_string, return_tensors="pt")

fn_generate = lambda x: model.generate_with_grad(
        inputs_embeds=x,
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=i_max_new_tokens,
        num_beams=i_num_beams,
        num_return_sequences=i_num_return,
        return_dict_in_generate=True,
        output_scores=True)

# MAIN --------------------------
if __name__ == "__main__":
  
    print(input_embedding.shape)
    
    input_embedding_start = torch.clone(input_embedding) # save it
    
    output_start = model.generate_with_grad(
        inputs_embeds=input_embedding_start,
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens=i_max_new_tokens,
        num_beams=i_num_beams,
        num_return_sequences=i_num_return,
        return_dict_in_generate=True,
        output_scores=True)
    
    print("START\n")
    print(input_embedding_start)
    print(tokenizer.decode(output_start.sequences[0]))
    print(output_start.sequences[0])
    
    ll = llikelihood(output_start, target_sequence)
    
    print(ll)
    make_dot(ll).render("dag", format="svg")   
    
    #gd = torch.optim.SGD([omega], lr=1e-5)
    gd = torch.optim.Adam([input_embedding], lr=0.001)
    history_gd = []

    for i in tqdm(range(10000)):
        gd.zero_grad()
        
        # generate output for the current input
        objective = llikelihood(fn_generate(input_embedding), target_sequence)
        objective.backward()
        gd.step()
        history_gd.append(objective.item())
        
        if (i > 1) and (i % 10 == 0):
            print(input_embedding)
            print(f"neg. LL: {history_gd[-1]}")
            print(f"Delta: {np.abs(history_gd[-1] - history_gd[-2])}")
        
        if (i>1) and (np.abs(history_gd[-1] - history_gd[-2]) < .01):
            print("Convergence achieved in ", i+1, " iterations")
            print("-LogL Value: ", objective.item())
            print("Mean |gradient|: ", torch.abs(input_embedding.grad).mean().item())
            break
    
    print("\n")
    print("\n")
    print("RESULT\n")
        
    output_final = fn_generate(input_embedding)
    
    print(input_embedding_start)
    print(input_embedding)
    
    print(output_start.scores)
    print(output_final.scores)
    
    print(torch.argmax(output_start.scores[0]))
    print(torch.argmax(output_final.scores[0]))
    
    print(output_start.sequences[0])
    print(output_final.sequences[0])
    print(target_sequence[0])

    print(tokenizer.decode(output_start.sequences[0]))
    print(tokenizer.decode(output_final.sequences[0]))
    print(tokenizer.decode(target_sequence[0]))