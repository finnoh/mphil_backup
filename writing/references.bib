@inproceedings{8215574,
  title = {Market Basket Prediction Using User-Centric Temporal Annotated Recurring Sequences},
  booktitle = {2017 {{IEEE}} International Conference on Data Mining ({{ICDM}})},
  author = {Guidotti, Riccardo and Rossetti, Giulio and Pappalardo, Luca and Giannotti, Fosca and Pedreschi, Dino},
  year = {2017},
  pages = {895--900},
  doi = {10.1109/ICDM.2017.111},
  keywords = {notion}
}

@article{aakerDimensionsBrandPersonality1997,
  title = {Dimensions of {{Brand Personality}}},
  author = {Aaker, Jennifer L.},
  year = {1997},
  month = aug,
  journal = {Journal of Marketing Research},
  volume = {34},
  number = {3},
  pages = {347--356},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/002224379703400304},
  urldate = {2023-10-12},
  abstract = {Although a considerable amount of research in personality psychology has been done to conceptualize human personality, identify the ``Big Five'' dimensions, and explore the meaning of each dimension, no parallel research has been conducted in consumer behavior on brand personality. Consequently, an understanding of the symbolic use of brands has been limited in the consumer behavior literature. In this research, the author develops a theoretical framework of the brand personality construct by determining the number and nature of dimensions of brand personality (Sincerity, Excitement, Competence, Sophistication, and Ruggedness). To measure the five brand personality dimensions, a reliable, valid, and generalizable measurement scale is created. Finally, theoretical and practical implications regarding the symbolic use of brands are discussed.},
  langid = {english},
  keywords = {notion}
}

@techreport{aflaki2016choosing,
  title = {Choosing to Be Strategic: {{Implications}} of the Endogenous Adoption of Forward-Looking Purchasing Behavior on Multiperiod Pricing},
  author = {Aflaki, Arian and Feldman, Pnina and Swinney, Robert},
  year = {2016},
  institution = {Working paper},
  keywords = {notion}
}

@article{amezagahechavarriaModifiedAttentionMechanism2022,
  title = {A Modified Attention Mechanism Powered by {{Bayesian Network}} for User Activity Analysis and Prediction},
  author = {Amezaga Hechavarria, Alexis and Shafiq, M. Omair},
  year = {2022},
  month = jul,
  journal = {Data \& Knowledge Engineering},
  volume = {140},
  pages = {102034},
  issn = {0169023X},
  doi = {10.1016/j.datak.2022.102034},
  urldate = {2023-09-26},
  langid = {english},
  keywords = {notion}
}

@article{anderlMappingCustomerJourney2016,
  title = {Mapping the Customer Journey: {{Lessons}} Learned from Graph-Based Online Attribution Modeling},
  shorttitle = {Mapping the Customer Journey},
  author = {Anderl, Eva and Becker, Ingo and Von Wangenheim, Florian and Schumann, Jan Hendrik},
  year = {2016},
  month = sep,
  journal = {International Journal of Research in Marketing},
  volume = {33},
  number = {3},
  pages = {457--474},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2016.03.001},
  urldate = {2023-09-26},
  langid = {english},
  keywords = {notion}
}

@book{anderson2010social,
  title = {Social Media Marketing: Game Theory and the Emergence of Collaboration},
  author = {Anderson, Eric},
  year = {2010},
  publisher = {Springer Science \& Business Media},
  keywords = {notion}
}

@article{andersonHarbingersFailure2015,
  title = {Harbingers of {{Failure}}},
  author = {Anderson, Eric and Lin, Song and Simester, Duncan and Tucker, Catherine},
  year = {2015},
  month = oct,
  journal = {Journal of Marketing Research},
  volume = {52},
  number = {5},
  pages = {580--592},
  issn = {0022-2437, 1547-7193},
  doi = {10.1509/jmr.13.0415},
  urldate = {2023-09-30},
  abstract = {The authors identify customers, termed ``Harbingers of failure,'' who systematically purchase new products that flop. Their early adoption of a new product is a strong signal that a product will fail---the more they buy, the less likely the product will succeed. Firms can identify these customers through past purchases of either new products that failed or existing products that few other customers purchase. The authors discuss how these insights can be readily incorporated into the new product development process. The findings challenge the conventional wisdom that positive customer feedback is always a signal of future success.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Downloads/retrieve.pdf;/Users/hoener/Zotero/storage/SXVURDVZ/Anderson et al. - 2015 - Harbingers of Failure.pdf}
}

@article{aparicioPriceFrictionsSuccess2022,
  title = {Price {{Frictions}} and the {{Success}} of {{New Products}}},
  author = {Aparicio, Diego and Simester, Duncan},
  year = {2022},
  month = nov,
  journal = {Marketing Science},
  volume = {41},
  number = {6},
  pages = {1057--1073},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2022.1367},
  urldate = {2023-10-09},
  abstract = {Price frictions reduce the success of new products and impact retailers' product assortments.           ,              The literature has documented that price frictions limit the frequency of price changes. We show that they are also associated with the failure of new products. Using the IRI academic data set, we identify new products that have low initial sales. When price frictions are high, retailers are less likely to adjust the price, and instead are more likely to discontinue the new product. We replicate the results by investigating retailer price and product line adjustments following the opening of a new store. We investigate three settings in which retailers may be reluctant to adjust prices on a new product: (a) when there are no price changes on related products, (b) when state-level pricing laws require price stickers on each package, and (c) when the initial prices end in 99Â¢. The sources of variation are very different across these settings, ranging from the timing of wholesale price changes to variation in state consumer protection laws to kinks in the demand curve associated with 99Â¢ price endings. Despite this diversity, our findings are consistent, suggesting that larger price frictions coincide with a higher probability that new items are discontinued when initial sales are low. This consistency leads us to conclude that it is more plausible that the effect is causal, rather than an artifact of endogeneity.             History: Avi Goldfarb served as the senior editor for this article and Christophe Van den Bulte served as the associate editor for this article.             Supplemental Material: The data and online appendix are available at https://doi.org/10.1287/mksc.2022.1367 .},
  langid = {english},
  keywords = {notion}
}

@article{ascarzaRetentionFutilityTargeting2018,
  title = {Retention {{Futility}}: {{Targeting High-Risk Customers Might}} Be {{Ineffective}}},
  shorttitle = {Retention {{Futility}}},
  author = {Ascarza, Eva},
  year = {2018},
  month = feb,
  journal = {Journal of Marketing Research},
  volume = {55},
  number = {1},
  pages = {80--98},
  issn = {0022-2437, 1547-7193},
  doi = {10.1509/jmr.16.0163},
  urldate = {2023-04-26},
  abstract = {Companies in a variety of sectors are increasingly managing customer churn proactively, generally by detecting customers at the highest risk of churning and targeting retention efforts towards them. While there is a vast literature on developing churn prediction models that identify customers at the highest risk of churning, no research has investigated whether it is indeed optimal to target those individuals. Combining two field experiments with machine learning techniques, the author demonstrates that customers identified as having the highest risk of churning are not necessarily the best targets for proactive churn programs. This finding is not only contrary to common wisdom but also suggests that retention programs are sometimes futile not because firms offer the wrong incentives but because they do not apply the right targeting rules. Accordingly, firms should focus their modeling efforts on identifying the observed heterogeneity in response to the intervention and to target customers on the basis of their sensitivity to the intervention, regardless of their risk of churning. This approach is empirically demonstrated to be significantly more effective than the standard practice of targeting customers with the highest risk of churning. More broadly, the author encourages firms and researchers using randomized trials (or A/B tests) to look beyond the average effect of interventions and leverage the observed heterogeneity in customers' response to select customer targets.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/QBRW2QF5/Ascarza - 2018 - Retention Futility Targeting High-Risk Customers .pdf}
}

@article{assadAlgorithmicPricingCompetition2020,
  title = {Algorithmic {{Pricing}} and {{Competition}}: {{Empirical Evidence}} from the {{German Retail Gasoline Market}}},
  shorttitle = {Algorithmic {{Pricing}} and {{Competition}}},
  author = {Assad, Stephanie and Clark, Robert and Ershov, Daniel and Xu, Lei},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3682021},
  urldate = {2023-10-17},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/ERIC9TV3/Assad et al. - 2020 - Algorithmic Pricing and Competition Empirical Evi.pdf}
}

@article{atheyEnsembleMethodsCausal2019,
  title = {Ensemble {{Methods}} for {{Causal Effects}} in {{Panel Data Settings}}},
  author = {Athey, Susan and Bayati, Mohsen and Imbens, Guido and Qu, Zhaonan},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1903.10079},
  urldate = {2023-10-09},
  abstract = {This paper studies a panel data setting where the goal is to estimate causal effects of an intervention by predicting the counterfactual values of outcomes for treated units, had they not received the treatment. Several approaches have been proposed for this problem, including regression methods, synthetic control methods and matrix completion methods. This paper considers an ensemble approach, and shows that it performs better than any of the individual methods in several economic datasets. Matrix completion methods are often given the most weight by the ensemble, but this clearly depends on the setting. We argue that ensemble methods present a fruitful direction for further research in the causal panel data setting.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Econometrics (econ.EM),FOS: Economics and business,notion}
}

@misc{audestadDynamicMarketModels2015,
  title = {Some {{Dynamic Market Models}}},
  author = {Audestad, Jan A.},
  year = {2015},
  month = nov,
  number = {arXiv:1511.07203},
  eprint = {1511.07203},
  primaryclass = {q-fin},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {In this text, we study the temporal behavior of markets using models expressible as ordinary differential equations. The markets studied are those where each customer buys only one copy of the good, for example, subscription of smartphone service, journals and newspapers, and goods such as books, music and games. The underlying model is the diffusion model of Frank Bass. Evolution of markets with no competitors and markets with several competitors are analyzed where, in particulat, the effects of churning upon the market evolution is investigated. Analytic solutions are given for the temporal evolution of several types of interactive games.},
  archiveprefix = {arxiv},
  keywords = {Economics - General Economics,notion},
  file = {/Users/hoener/Zotero/storage/RAT4LUZI/Audestad - 2015 - Some Dynamic Market Models.pdf;/Users/hoener/Zotero/storage/APMHB4FI/1511.html}
}

@inproceedings{baiAttributeawareNeuralAttentive2018,
  title = {An {{Attribute-aware Neural Attentive Model}} for {{Next Basket Recommendation}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Bai, Ting and Nie, Jian-Yun and Zhao, Wayne Xin and Zhu, Yutao and Du, Pan and Wen, Ji-Rong},
  year = {2018},
  month = jun,
  pages = {1201--1204},
  publisher = {ACM},
  address = {Ann Arbor MI USA},
  doi = {10.1145/3209978.3210129},
  urldate = {2023-06-25},
  isbn = {978-1-4503-5657-2},
  langid = {english},
  keywords = {notion}
}

@inproceedings{baltescuItemSageLearningProduct2022,
  title = {{{ItemSage}}: {{Learning Product Embeddings}} for {{Shopping Recommendations}} at {{Pinterest}}},
  shorttitle = {{{ItemSage}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Baltescu, Paul and Chen, Haoyu and Pancha, Nikil and Zhai, Andrew and Leskovec, Jure and Rosenberg, Charles},
  year = {2022},
  month = aug,
  pages = {2703--2711},
  publisher = {ACM},
  address = {Washington DC USA},
  doi = {10.1145/3534678.3539170},
  urldate = {2023-07-11},
  isbn = {978-1-4503-9385-0},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/YRLR8GQG/Baltescu et al. - 2022 - ItemSage Learning Product Embeddings for Shopping.pdf}
}

@article{belParameterEstimationMultivariate2018,
  title = {Parameter Estimation in Multivariate Logit Models with Many Binary Choices},
  author = {Bel, Koen and Fok, Dennis and Paap, Richard},
  year = {2018},
  month = may,
  journal = {Econometric Reviews},
  volume = {37},
  number = {5},
  pages = {534--550},
  issn = {0747-4938, 1532-4168},
  doi = {10.1080/07474938.2015.1093780},
  urldate = {2023-05-09},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/ZHT6TK4P/Bel et al. - 2018 - Parameter estimation in multivariate logit models .pdf}
}

@inproceedings{benderDangersStochasticParrots2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? ðŸ¦œ},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  pages = {610--623},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3442188.3445922},
  urldate = {2023-06-26},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/I8MP3WNP/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf}
}

@misc{bengioMetaTransferObjectiveLearning2019,
  title = {A {{Meta-Transfer Objective}} for {{Learning}} to {{Disentangle Causal Mechanisms}}},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  year = {2019},
  month = feb,
  number = {arXiv:1901.10912},
  eprint = {1901.10912},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/hoener/Zotero/storage/5JQ44G8U/Bengio et al. - 2019 - A Meta-Transfer Objective for Learning to Disentan.pdf;/Users/hoener/Zotero/storage/FE68PNYK/1901.html}
}

@article{bergerUnitingTribesUsing2020,
  title = {Uniting the {{Tribes}}: {{Using Text}} for {{Marketing Insight}}},
  shorttitle = {Uniting the {{Tribes}}},
  author = {Berger, Jonah and Humphreys, Ashlee and Ludwig, Stephan and Moe, Wendy W. and Netzer, Oded and Schweidel, David A.},
  year = {2020},
  month = jan,
  journal = {Journal of Marketing},
  volume = {84},
  number = {1},
  pages = {1--25},
  issn = {0022-2429, 1547-7185},
  doi = {10.1177/0022242919873106},
  urldate = {2023-04-26},
  abstract = {Words are part of almost every marketplace interaction. Online reviews, customer service calls, press releases, marketing communications, and other interactions create a wealth of textual data. But how can marketers best use such data? This article provides an overview of automated textual analysis and details how it can be used to generate marketing insights. The authors discuss how text reflects qualities of the text producer (and the context in which the text was produced) and impacts the audience or text recipient. Next, they discuss how text can be a powerful tool both for prediction and for understanding (i.e., insights). Then, the authors overview methodologies and metrics used in text analysis, providing a set of guidelines and procedures. Finally, they further highlight some common metrics and challenges and discuss how researchers can address issues of internal and external validity. They conclude with a discussion of potential areas for future work. Along the way, the authors note how textual analysis can unite the tribes of marketing. While most marketing problems are interdisciplinary, the field is often fragmented. By involving skills and ideas from each of the subareas of marketing, text analysis has the potential to help unite the field with a common set of tools and approaches.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/9LPMJSHT/Berger et al. - 2020 - Uniting the Tribes Using Text for Marketing Insig.pdf}
}

@article{blattberg1995promotions,
  title = {How Promotions Work},
  author = {Blattberg, Robert C and Briesch, Richard and Fox, Edward J},
  year = {1995},
  journal = {Marketing science},
  volume = {14},
  number = {3\_supplement},
  pages = {G122--G132},
  publisher = {INFORMS},
  keywords = {notion}
}

@article{blei2007correlated,
  title = {A Correlated Topic Model of Science},
  author = {Blei, David M and Lafferty, John D},
  year = {2007},
  keywords = {notion}
}

@misc{borisovLanguageModelsAre2023,
  title = {Language {{Models}} Are {{Realistic Tabular Data Generators}}},
  author = {Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2023},
  month = apr,
  number = {arXiv:2210.06280},
  eprint = {2210.06280},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/hoener/Zotero/storage/8P9LQI9A/Borisov et al. - 2023 - Language Models are Realistic Tabular Data Generat.pdf;/Users/hoener/Zotero/storage/L2Y7XF9F/2210.html}
}

@article{borreroElevatingUnivariateTime2023,
  title = {Elevating {{Univariate Time Series Forecasting}}: {{Innovative SVR-Empowered Nonlinear Autoregressive Neural Networks}}},
  shorttitle = {Elevating {{Univariate Time Series Forecasting}}},
  author = {Borrero, Juan D. and Mariscal, Jesus},
  year = {2023},
  month = sep,
  journal = {Algorithms},
  volume = {16},
  number = {9},
  pages = {423},
  issn = {1999-4893},
  doi = {10.3390/a16090423},
  urldate = {2023-10-12},
  abstract = {Efforts across diverse domains like economics, energy, and agronomy have focused on developing predictive models for time series data. A spectrum of techniques, spanning from elementary linear models to intricate neural networks and machine learning algorithms, has been explored to achieve accurate forecasts. The hybrid ARIMA-SVR model has garnered attention due to its fusion of a foundational linear model with error correction capabilities. However, its use is limited to stationary time series data, posing a significant challenge. To overcome these limitations and drive progress, we propose the innovative NAR--SVR hybrid method. Unlike its predecessor, this approach breaks free from stationarity and linearity constraints, leading to improved model performance solely through historical data exploitation. This advancement significantly reduces the time and computational resources needed for precise predictions, a critical factor in univariate economic time series forecasting. We apply the NAR--SVR hybrid model in three scenarios: Spanish berry daily yield data from 2018 to 2021, daily COVID-19 cases in three countries during 2020, and the daily Bitcoin price time series from 2015 to 2020. Through extensive comparative analyses with other time series prediction models, our results substantiate that our novel approach consistently outperforms its counterparts. By transcending stationarity and linearity limitations, our hybrid methodology establishes a new paradigm for univariate time series forecasting, revolutionizing the field and enhancing predictive capabilities across various domains as highlighted in this study.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/85I8L3SX/Borrero and Mariscal - 2023 - Elevating Univariate Time Series Forecasting Inno.pdf}
}

@article{boztugCombinedApproachSegmentspecific2008,
  title = {A Combined Approach for Segment-Specific Market Basket Analysis},
  author = {Boztu{\u g}, Yasemin and Reutterer, Thomas},
  year = {2008},
  month = may,
  journal = {European Journal of Operational Research},
  volume = {187},
  number = {1},
  pages = {294--312},
  issn = {03772217},
  doi = {10.1016/j.ejor.2007.03.001},
  urldate = {2023-06-26},
  langid = {english},
  keywords = {notion}
}

@inproceedings{cervino2023learning,
  title = {Learning Globally Smooth Functions on Manifolds},
  booktitle = {International Conference on Machine Learning},
  author = {Cervino, Juan and Chamon, Luiz FO and Haeffele, Benjamin David and Vidal, Rene and Ribeiro, Alejandro},
  year = {2023},
  pages = {3815--3854},
  publisher = {PMLR},
  keywords = {notion}
}

@article{chenProduct2VecUnderstandingProductLevel2020,
  title = {{{Product2Vec}}: {{Understanding Product-Level Competition Using Representation Learning}}},
  shorttitle = {{{Product2Vec}}},
  author = {Chen, Fanglin and Liu, Xiao and Proserpio, Davide and Troncoso, Isamar},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3519358},
  urldate = {2023-09-26},
  langid = {english},
  keywords = {notion}
}

@article{chungDynamicAdvertisingEffect2013,
  title = {The {{Dynamic Advertising Effect}} of {{Collegiate Athletics}}},
  author = {Chung, Doug J.},
  year = {2013},
  month = sep,
  journal = {Marketing Science},
  volume = {32},
  number = {5},
  pages = {679--698},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2013.0795},
  urldate = {2023-10-14},
  abstract = {I measure the spillover effect of intercollegiate athletics on the quantity and quality of applicants to institutions of higher education in the United States---an effect popularly known as the ``Flutie effect.'' I treat athletic success as a stock of goodwill that decays over time, similar to that of advertising. A major challenge is that privacy laws prevent us from observing information about the applicant pool. I overcome this challenge by using order statistic distribution to infer applicant quality from information on enrolled students. Using a flexible random-coefficients aggregate discrete choice model that accommodates heterogeneity in preferences for school quality and athletic success, as well as an extensive set of school fixed effects to control for unobserved quality in athletics and academics, I estimate the impact of athletic success on applicant quality and quantity. Overall, athletic success has a significant, long-term goodwill effect on future applications and quality. However, students with lower-than-average SAT scores tend to have a stronger preference for athletic success, whereas students with higher SAT scores have a greater preference for academic quality. Furthermore, the decay rate of athletics' goodwill is significant only for students with lower SAT scores, suggesting that the goodwill created by intercollegiate athletics resides more extensively with lower-scoring students than with their higher-scoring counterparts. But, surprisingly, athletic success impacts applications even among academically stronger students.},
  langid = {english},
  keywords = {notion}
}

@article{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1412.3555},
  urldate = {2023-05-09},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Neural and Evolutionary Computing (cs.NE),notion},
  file = {/Users/hoener/Zotero/storage/R4FMLTE9/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf}
}

@article{cuiStableLearningEstablishes2022,
  title = {Stable Learning Establishes Some Common Ground between Causal Inference and Machine Learning},
  author = {Cui, Peng and Athey, Susan},
  year = {2022},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {2},
  pages = {110--115},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00445-z},
  urldate = {2023-10-15},
  langid = {english},
  keywords = {notion}
}

@book{demeniconiProceedings2020SIAM2020,
  title = {Proceedings of the 2020 {{SIAM International Conference}} on {{Data Mining}}},
  editor = {Demeniconi, Carlotta and Chawla, Nitesh},
  year = {2020},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA},
  doi = {10.1137/1.9781611976236},
  urldate = {2023-05-09},
  isbn = {978-1-61197-623-6},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/HBW54PAR/Demeniconi and Chawla - 2020 - Proceedings of the 2020 SIAM International Confere.pdf}
}

@article{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1810.04805},
  urldate = {2023-07-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,notion}
}

@article{dewModelingDynamicHeterogeneity2020,
  title = {Modeling {{Dynamic Heterogeneity Using Gaussian Processes}}},
  author = {Dew, Ryan and Ansari, Asim and Li, Yang},
  year = {2020},
  month = feb,
  journal = {Journal of Marketing Research},
  volume = {57},
  number = {1},
  pages = {55--77},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/0022243719874047},
  urldate = {2023-05-22},
  abstract = {Marketing research relies on individual-level estimates to understand the rich heterogeneity of consumers, firms, and products. While much of the literature focuses on capturing static cross-sectional heterogeneity, little research has been done on modeling dynamic heterogeneity, or the heterogeneous evolution of individual-level model parameters. In this work, the authors propose a novel framework for capturing the dynamics of heterogeneity, using individual-level, latent, Bayesian nonparametric Gaussian processes. Similar to standard heterogeneity specifications, this Gaussian process dynamic heterogeneity (GPDH) specification models individual-level parameters as flexible variations around population-level trends, allowing for sharing of statistical information both across individuals and within individuals over time. This hierarchical structure provides precise individual-level insights regarding parameter dynamics. The authors show that GPDH nests existing heterogeneity specifications and that not flexibly capturing individual-level dynamics may result in biased parameter estimates. Substantively, they apply GPDH to understand preference dynamics and to model the evolution of online reviews. Across both applications, they find robust evidence of dynamic heterogeneity and illustrate GPDH's rich managerial insights, with implications for targeting, pricing, and market structure analysis.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Downloads/dewModelingDynamicHeterogeneity2020.pdf;/Users/hoener/Zotero/storage/CRJCLW5T/Dew et al. - 2020 - Modeling Dynamic Heterogeneity Using Gaussian Proc.pdf}
}

@article{dherin2022neural,
  title = {Why Neural Networks Find Simple Solutions: {{The}} Many Regularizers of Geometric Complexity},
  author = {Dherin, Benoit and Munn, Michael and Rosca, Mihaela and Barrett, David},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {2333--2349},
  keywords = {notion}
}

@article{diclementeSequencesPurchasesCredit2018,
  title = {Sequences of Purchases in Credit Card Data Reveal Lifestyles in Urban Populations},
  author = {Di Clemente, Riccardo and {Luengo-Oroz}, Miguel and Travizano, Matias and Xu, Sharon and Vaitla, Bapu and Gonz{\'a}lez, Marta C.},
  year = {2018},
  month = aug,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {3330},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-05690-8},
  urldate = {2023-10-09},
  abstract = {Abstract             Zipf-like distributions characterize a wide set of phenomena in physics, biology, economics, and social sciences. In human activities, Zipf's law describes, for example, the frequency of appearance of words in a text or the purchase types in shopping patterns. In the latter, the uneven distribution of transaction types is bound with the temporal sequences of purchases of individual choices. In this work, we define a framework using a text compression technique on the sequences of credit card purchases to detect ubiquitous patterns of collective behavior. Clustering the consumers by their similarity in purchase sequences, we detect five consumer groups. Remarkably, post checking, individuals in each group are also similar in their age, total expenditure, gender, and the diversity of their social and mobility networks extracted from their mobile phone records. By properly deconstructing transaction data with Zipf-like distributions, this method uncovers sets of significant sequences that reveal insights on collective human behavior.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/VV8DQYT4/Di Clemente et al. - 2018 - Sequences of purchases in credit card data reveal .pdf}
}

@misc{dolphinStockEmbeddingsLearning2022,
  title = {Stock {{Embeddings}}: {{Learning Distributed Representations}} for {{Financial Assets}}},
  shorttitle = {Stock {{Embeddings}}},
  author = {Dolphin, Rian and Smyth, Barry and Dong, Ruihai},
  year = {2022},
  month = feb,
  number = {arXiv:2202.08968},
  eprint = {2202.08968},
  primaryclass = {cs, q-fin},
  publisher = {arXiv},
  urldate = {2023-10-16},
  abstract = {Identifying meaningful relationships between the price movements of financial assets is a challenging but important problem in a variety of financial applications. However with recent research, particularly those using machine learning and deep learning techniques, focused mostly on price forecasting, the literature investigating the modelling of asset correlations has lagged somewhat. To address this, inspired by recent successes in natural language processing, we propose a neural model for training stock embeddings, which harnesses the dynamics of historical returns data in order to learn the nuanced relationships that exist between financial assets. We describe our approach in detail and discuss a number of ways that it can be used in the financial domain. Furthermore, we present the evaluation results to demonstrate the utility of this approach, compared to several important benchmarks, in two real-world financial analytics tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Quantitative Finance - Computational Finance,Quantitative Finance - Statistical Finance},
  file = {/Users/hoener/Zotero/storage/2BKG47W5/Dolphin et al. - 2022 - Stock Embeddings Learning Distributed Representat.pdf;/Users/hoener/Zotero/storage/TL29RHZI/2202.html}
}

@article{dongesComplexNetworksClimate2009,
  title = {Complex Networks in Climate Dynamics - {{Comparing}} Linear and Nonlinear Network Construction Methods},
  author = {Donges, Jonathan F. and Zou, Yong and Marwan, Norbert and Kurths, J{\"u}rgen},
  year = {2009},
  month = jul,
  journal = {The European Physical Journal Special Topics},
  volume = {174},
  number = {1},
  eprint = {0907.4359},
  primaryclass = {physics},
  pages = {157--179},
  issn = {1951-6355, 1951-6401},
  doi = {10.1140/epjst/e2009-01098-2},
  urldate = {2023-10-16},
  abstract = {Complex network theory provides a powerful framework to statistically investigate the topology of local and non-local statistical interrelationships, i.e. teleconnections, in the climate system. Climate networks constructed from the same global climatological data set using the linear Pearson correlation coefficient or the nonlinear mutual information as a measure of dynamical similarity between regions, are compared systematically on local, mesoscopic and global topological scales. A high degree of similarity is observed on the local and mesoscopic topological scales for surface air temperature fields taken from AOGCM and reanalysis data sets. We find larger differences on the global scale, particularly in the betweenness centrality field. The global scale view on climate networks obtained using mutual information offers promising new perspectives for detecting network structures based on nonlinear physical processes in the climate system.},
  archiveprefix = {arxiv},
  keywords = {notion,Physics - Atmospheric and Oceanic Physics,Physics - Data Analysis Statistics and Probability},
  file = {/Users/hoener/Zotero/storage/XPG9PB82/Donges et al. - 2009 - Complex networks in climate dynamics - Comparing l.pdf;/Users/hoener/Zotero/storage/6QZXEEUN/0907.html}
}

@article{donnellyCounterfactualInferenceConsumer2019,
  title = {Counterfactual {{Inference}} for {{Consumer Choice Across Many Product Categories}}},
  author = {Donnelly, Rob and Ruiz, Francisco R. and Blei, David and Athey, Susan},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1906.02635},
  urldate = {2023-10-09},
  abstract = {This paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in parallel. The consumer's utility is additive in the different categories. Her preferences about product attributes as well as her price sensitivity vary across products and are in general correlated across products. We build on techniques from the machine learning literature on probabilistic models of matrix factorization, extending the methods to account for time-varying product attributes and products going out of stock. We evaluate the performance of the model using held-out data from weeks with price changes or out of stock products. We show that our model improves over traditional modeling approaches that consider each category in isolation. One source of the improvement is the ability of the model to accurately estimate heterogeneity in preferences (by pooling information across categories); another source of improvement is its ability to estimate the preferences of consumers who have rarely or never made a purchase in a given category in the training data. Using held-out data, we show that our model can accurately distinguish which consumers are most price sensitive to a given product. We consider counterfactuals such as personally targeted price discounts, showing that using a richer model such as the one we propose substantially increases the benefits of personalization in discounts.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Econometrics (econ.EM),FOS: Computer and information sciences,FOS: Economics and business,Machine Learning (cs.LG),Machine Learning (stat.ML),notion}
}

@article{donnellyCounterfactualInferenceConsumer2021,
  title = {Counterfactual Inference for Consumer Choice across Many Product Categories},
  author = {Donnelly, Robert and Ruiz, Francisco J.R. and Blei, David and Athey, Susan},
  year = {2021},
  month = dec,
  journal = {Quantitative Marketing and Economics},
  volume = {19},
  number = {3-4},
  pages = {369--407},
  issn = {1570-7156, 1573-711X},
  doi = {10.1007/s11129-021-09241-2},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/Q7GNQEFK/Donnelly et al. - 2021 - Counterfactual inference for consumer choice acros.pdf}
}

@article{doroticRewardRedemptionEffects2014,
  title = {Reward Redemption Effects in a Loyalty Program When Customers Choose How Much and When to Redeem},
  author = {Dorotic, Matilda and Verhoef, Peter C. and Fok, Dennis and Bijmolt, Tammo H.A.},
  year = {2014},
  month = dec,
  journal = {International Journal of Research in Marketing},
  volume = {31},
  number = {4},
  pages = {339--355},
  issn = {01678116},
  doi = {10.1016/j.ijresmar.2014.06.001},
  urldate = {2023-04-26},
  abstract = {The redemption of loyalty program (LP) rewards has an important impact on LP members' behavior, particularly on purchase behavior before and after redeeming a reward. However, little is known about the interplay between members' purchase and redemption behavior when members are not pressured with point expiration and they choose for themselves when and how much to redeem. In this context, the effects of redemption are not straightforward, as little additional effort is required from an LP member to obtain the reward. Analyzing the behavior of 3094 members in such an LP, we find that the mere decision to redeem a reward significantly enhances purchase behavior before and after the redemption event, even when members redeem just a fraction of their accumulated points. Conceptually, we refer to this enhancement as the redemption momentum, which is an alternative and novel explanation of the existence of pre-reward effects that do not depend on points-pressure. In addition to the overall impact of redemption on purchases, prior purchase behavior also enhances redemption decisions. Finally, we find a number of moderating effects on purchase and redemption behavior that derive from the length of LP membership, age, income and direct mailings. Our study's most important managerial implication is that firms should avoid imposing point expiry and/or binding thresholds in order to enhance members' purchase behavior.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/7FKSG369/Dorotic et al. - 2014 - Reward redemption effects in a loyalty program whe.pdf}
}

@misc{duTorchChoicePyTorchPackage2023,
  title = {Torch-{{Choice}}: {{A PyTorch Package}} for {{Large-Scale Choice Modelling}} with {{Python}}},
  shorttitle = {Torch-{{Choice}}},
  author = {Du, Tianyu and Kanodia, Ayush and Athey, Susan},
  year = {2023},
  month = jul,
  number = {arXiv:2304.01906},
  eprint = {2304.01906},
  primaryclass = {cs, econ},
  publisher = {arXiv},
  urldate = {2023-10-15},
  abstract = {The \${\textbackslash}texttt\{torch-choice\}\$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. \${\textbackslash}texttt\{torch-choice\}\$ provides a \${\textbackslash}texttt\{ChoiceDataset\}\$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a \${\textbackslash}texttt\{ChoiceDataset\}\$ from databases of various formats and functionalities of \${\textbackslash}texttt\{ChoiceDataset\}\$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of \${\textbackslash}texttt\{torch-choice\}\$ and \${\textbackslash}texttt\{mlogit\}\$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) the expansion of item sets. Finally, we demonstrate the scalability of \${\textbackslash}texttt\{torch-choice\}\$ on large-scale datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Economics - Econometrics,notion},
  file = {/Users/hoener/Zotero/storage/QZ4GWQXZ/Du et al. - 2023 - Torch-Choice A PyTorch Package for Large-Scale Ch.pdf;/Users/hoener/Zotero/storage/MDWDNP4D/2304.html}
}

@article{eggenbergerITSkillsOccupation2023,
  title = {{{IT}} Skills, Occupation Specificity and Job Separations},
  author = {Eggenberger, Christian and {Backes-Gellner}, Uschi},
  year = {2023},
  month = feb,
  journal = {Economics of Education Review},
  volume = {92},
  pages = {102333},
  issn = {02727757},
  doi = {10.1016/j.econedurev.2022.102333},
  urldate = {2023-04-26},
  abstract = {This paper examines how workers' earnings change after involuntary job separations depending on the workers' acquired IT skills and the specificity of their occupational training. We categorize workers' occupational skill bundles along two independent dimensions. First, we distinguish between skill bundles that are more specific or less specific compared to the skill bundles needed in the overall labor market. Second, as digitalization becomes ever more important, we distinguish between skill bundles that contain two different types of IT skills, generic or expert IT skills. We expect that after involuntary separations, these different types of IT skills can have opposing effects, either reducing or amplifying earnings losses of workers with specific skill bundles. We find clearly opposing results for workers in specific occupations---but not in general occupations: Having more generic IT skills is positively correlated with earnings after involuntary separations, whereas more expert IT skills is negatively correlated.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/776I8R6J/Eggenberger and Backes-Gellner - 2023 - IT skills, occupation specificity and job separati.pdf}
}

@article{fanSequentialRecommendationAuxiliary2022,
  title = {Sequential {{Recommendation}} with {{Auxiliary Item Relationships}} via {{Multi-Relational Transformer}}},
  author = {Fan, Ziwei and Liu, Zhiwei and Wang, Chen and Huang, Peijie and Peng, Hao and Yu, Philip S.},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.13572},
  urldate = {2023-05-09},
  abstract = {Sequential Recommendation (SR) models user dynamics and predicts the next preferred items based on the user history. Existing SR methods model the 'was interacted before' item-item transitions observed in sequences, which can be viewed as an item relationship. However, there are multiple auxiliary item relationships, e.g., items from similar brands and with similar contents in real-world scenarios. Auxiliary item relationships describe item-item affinities in multiple different semantics and alleviate the long-lasting cold start problem in the recommendation. However, it remains a significant challenge to model auxiliary item relationships in SR. To simultaneously model high-order item-item transitions in sequences and auxiliary item relationships, we propose a Multi-relational Transformer capable of modeling auxiliary item relationships for SR (MT4SR). Specifically, we propose a novel self-attention module, which incorporates arbitrary item relationships and weights item relationships accordingly. Second, we regularize intra-sequence item relationships with a novel regularization module to supervise attentions computations. Third, for inter-sequence item relationship pairs, we introduce a novel inter-sequence related items modeling module. Finally, we conduct experiments on four benchmark datasets and demonstrate the effectiveness of MT4SR over state-of-the-art methods and the improvements on the cold start problem. The code is available at https://github.com/zfan20/MT4SR.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Information Retrieval (cs.IR),Machine Learning (cs.LG),notion}
}

@inproceedings{fanSequentialRecommendationStochastic2022,
  title = {Sequential {{Recommendation}} via {{Stochastic Self-Attention}}},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {Fan, Ziwei and Liu, Zhiwei and Wang, Yu and Wang, Alice and Nazari, Zahra and Zheng, Lei and Peng, Hao and Yu, Philip S.},
  year = {2022},
  month = apr,
  pages = {2036--2047},
  publisher = {ACM},
  address = {Virtual Event, Lyon France},
  doi = {10.1145/3485447.3512077},
  urldate = {2023-05-09},
  isbn = {978-1-4503-9096-5},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/Z6SUJX6J/Fan et al. - 2022 - Sequential Recommendation via Stochastic Self-Atte.pdf}
}

@article{freemanMultidimensionalInteractiveFixedEffects2022,
  title = {Multidimensional {{Interactive Fixed-Effects}}},
  author = {Freeman, Hugo},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2209.11691},
  urldate = {2023-10-09},
  abstract = {This paper studies a linear and additively separable model for multidimensional panel data of three or more dimensions with unobserved interactive fixed effects. Two approaches are considered to account for these unobserved interactive fixed-effects when estimating coefficients on the observed covariates. First, the model is embedded within the standard two-dimensional panel framework and restrictions are derived under which the factor structure methods in Bai (2009) lead to consistent estimation of model parameters, but at potentially slow rates of convergence. The second approach utilises popular machine learning techniques to develop group fixed-effects and kernel weighted fixed-effects that are more robust to the multidimensional nature of the problem and can achieve the parametric rate of consistency under certain conditions. Theoretical results and simulations show the benefit of standard two-dimensional panel methods when the structure of the interactive fixed-effect term is known, but also highlight how the group fixed-effects and kernel methods perform well without knowledge of this structure. The methods are implemented to estimate the demand elasticity for beer under a handful of models for demand.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Econometrics (econ.EM),FOS: Computer and information sciences,FOS: Economics and business,Machine Learning (cs.LG),Methodology (stat.ME),notion}
}

@article{freemanMultidimensionalInteractiveFixedEffects2022a,
  title = {Multidimensional {{Interactive Fixed-Effects}}},
  author = {Freeman, Hugo},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2209.11691},
  urldate = {2023-10-12},
  abstract = {This paper studies a linear and additively separable model for multidimensional panel data of three or more dimensions with unobserved interactive fixed effects. Two approaches are considered to account for these unobserved interactive fixed-effects when estimating coefficients on the observed covariates. First, the model is embedded within the standard two-dimensional panel framework and restrictions are derived under which the factor structure methods in Bai (2009) lead to consistent estimation of model parameters, but at potentially slow rates of convergence. The second approach utilises popular machine learning techniques to develop group fixed-effects and kernel weighted fixed-effects that are more robust to the multidimensional nature of the problem and can achieve the parametric rate of consistency under certain conditions. Theoretical results and simulations show the benefit of standard two-dimensional panel methods when the structure of the interactive fixed-effect term is known, but also highlight how the group fixed-effects and kernel methods perform well without knowledge of this structure. The methods are implemented to estimate the demand elasticity for beer under a handful of models for demand.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Econometrics (econ.EM),FOS: Computer and information sciences,FOS: Economics and business,Machine Learning (cs.LG),Methodology (stat.ME),notion}
}

@article{fujii2021learning,
  title = {Learning Interaction Rules from Multi-Animal Trajectories via Augmented Behavioral Models},
  author = {Fujii, Keisuke and Takeishi, Naoya and Tsutsui, Kazushi and Fujioka, Emyo and Nishiumi, Nozomi and Tanaka, Ryoya and Fukushiro, Mika and Ide, Kaoru and Kohno, Hiroyoshi and Yoda, Ken and others},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {11108--11122},
  keywords = {notion}
}

@article{gabelP2VMAPMappingMarket2019,
  title = {{{P2V-MAP}}: {{Mapping Market Structures}} for {{Large Retail Assortments}}},
  shorttitle = {{{P2V-MAP}}},
  author = {Gabel, Sebastian and Guhl, Daniel and Klapper, Daniel},
  year = {2019},
  month = aug,
  journal = {Journal of Marketing Research},
  volume = {56},
  number = {4},
  pages = {557--580},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/0022243719833631},
  urldate = {2023-05-10},
  abstract = {The authors propose a new, exploratory approach for analyzing market structures that leverages two recent methodological advances in natural language processing and machine learning. They customize a neural network language model to derive latent product attributes by analyzing the co-occurrences of products in shopping baskets. Applying dimensionality reduction to the latent attributes yields a two-dimensional product map. This method is well-suited to retailers because it relies on data that are readily available from their checkout systems and facilitates their analyses of cross-category product complementarity, in addition to within-category substitution. The approach has high usability because it is automated, is scalable and does not require a priori assumptions. Its results are easy to interpret and update as new market basket data are collected. The authors validate their approach both by conducting an extensive simulation study and by comparing their results with those of state-of-the-art, econometric methods for modeling product relationships. The application of this approach using data collected at a leading German grocery retailer underlines its usefulness and provides novel findings that are relevant to assortment-related decisions.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/TPZGL4YV/Gabel et al. - 2019 - P2V-MAP Mapping Market Structures for Large Retai.pdf}
}

@article{gabelProductChoiceLarge2022,
  title = {Product {{Choice}} with {{Large Assortments}}: {{A Scalable Deep-Learning Model}}},
  shorttitle = {Product {{Choice}} with {{Large Assortments}}},
  author = {Gabel, Sebastian and Timoshenko, Artem},
  year = {2022},
  month = mar,
  journal = {Management Science},
  volume = {68},
  number = {3},
  pages = {1808--1827},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.2021.3969},
  urldate = {2023-05-09},
  abstract = {Personalized marketing in retail requires a model to predict how different marketing actions affect product choices by individual customers. Large retailers often handle millions of transactions daily, involving thousands of products in hundreds of categories. Product choice models thus need to scale to large product assortments and customer bases, without extensive product attribute information. To address these challenges, we propose a custom deep neural network model. The model incorporates bottleneck layers to encode cross-product relationships, calibrates time-series filters to capture purchase dynamics for products with different interpurchase times, and relies on weight sharing between the products to improve convergence and scale to large assortments. The model applies to loyalty card transaction data without predefined categories or product attributes to predict customer-specific purchase probabilities in response to marketing actions. In a simulation, the proposed product choice model predicts purchase decisions better than baseline methods by adjusting the predicted probabilities for the effects of recent purchases and price discounts. The improved predictions lead to substantially higher revenue gains in a simulated coupon personalization problem. We verify predictive performance using transaction data from a large retailer with experimental variation in price discounts.             This paper was accepted by Gui Liberali, Management Science Special Section on Data-Driven Prescriptive Analytics.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Downloads/gabelProductChoiceLarge2022.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Publisher's description},
  isbn = {978-0-262-33737-3},
  langid = {english},
  keywords = {notion},
  annotation = {OCLC: 1183962587},
  file = {/Users/hoener/Downloads/goodfellowDeepLearning2016.pdf}
}

@article{gupta1988impact,
  title = {Impact of Sales Promotions on When, What, and How Much to Buy},
  author = {Gupta, Sunil},
  year = {1988},
  journal = {Journal of Marketing research},
  volume = {25},
  number = {4},
  pages = {342--355},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  keywords = {notion}
}

@article{hamiltonConsumerJourneysDeveloping2019,
  title = {Consumer Journeys: Developing Consumer-Based Strategy},
  shorttitle = {Consumer Journeys},
  author = {Hamilton, Rebecca and Price, Linda L.},
  year = {2019},
  month = mar,
  journal = {Journal of the Academy of Marketing Science},
  volume = {47},
  number = {2},
  pages = {187--191},
  issn = {0092-0703, 1552-7824},
  doi = {10.1007/s11747-019-00636-y},
  urldate = {2023-07-10},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/SKF7454A/Hamilton and Price - 2019 - Consumer journeys developing consumer-based strat.pdf}
}

@book{haucap2015price,
  title = {Price Dispersion and Station Heterogeneity on {{German}} Retail Gasoline Markets},
  author = {Haucap, Justus and Heimeshoff, Ulrich and Siekmann, Manuel},
  year = {2015},
  number = {171},
  publisher = {DICE discussion paper},
  keywords = {notion}
}

@misc{hendriksLinearlyConstrainedNeural2021,
  title = {Linearly {{Constrained Neural Networks}}},
  author = {Hendriks, Johannes and Jidling, Carl and Wills, Adrian and Sch{\"o}n, Thomas},
  year = {2021},
  month = apr,
  number = {arXiv:2002.01600},
  eprint = {2002.01600},
  primaryclass = {physics, stat},
  publisher = {arXiv},
  urldate = {2023-10-17},
  abstract = {We present a novel approach to modelling and learning vector fields from physical systems using neural networks that explicitly satisfy known linear operator constraints. To achieve this, the target function is modelled as a linear transformation of an underlying potential field, which is in turn modelled by a neural network. This transformation is chosen such that any prediction of the target function is guaranteed to satisfy the constraints. The approach is demonstrated on both simulated and real data examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/Users/hoener/Zotero/storage/TWHKQZ4H/Hendriks et al. - 2021 - Linearly Constrained Neural Networks.pdf;/Users/hoener/Zotero/storage/WMLB9DTP/2002.html}
}

@article{hofmanIntegratingExplanationPrediction2021a,
  title = {Integrating Explanation and Prediction in Computational Social Science},
  author = {Hofman, Jake M. and Watts, Duncan J. and Athey, Susan and Garip, Filiz and Griffiths, Thomas L. and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J. and Vazire, Simine and Vespignani, Alessandro and Yarkoni, Tal},
  year = {2021},
  month = jul,
  journal = {Nature},
  volume = {595},
  number = {7866},
  pages = {181--188},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03659-0},
  urldate = {2023-10-15},
  langid = {english},
  keywords = {notion}
}

@book{hoyer2012consumer,
  title = {Consumer Behavior},
  author = {Hoyer, Wayne D and MacInnis, Deborah J and Pieters, Rik},
  year = {2012},
  publisher = {Cengage learning},
  keywords = {notion}
}

@article{imbensBreimanTwoCultures2021a,
  title = {Breiman's {{Two Cultures}}: {{A Perspective}} from {{Econometrics}}},
  shorttitle = {Breiman's {{Two Cultures}}},
  author = {Imbens, Guido and Athey, Susan},
  year = {2021},
  journal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {127--133},
  issn = {2767-3324},
  doi = {10.1353/obs.2021.0028},
  urldate = {2023-10-18},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/PVPK84TL/Imbens and Athey - 2021 - Breiman's Two Cultures A Perspective from Econome.pdf}
}

@article{ingaleLiteratureReviewPerformance2022,
  title = {Literature {{Review}} on Performance {{Evaluation}} of Recommendation System with Different Dimensions of Metrics},
  author = {Ingale, Vinod and Ellambotla, Saikiran},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4140551},
  urldate = {2023-05-09},
  langid = {english},
  keywords = {notion}
}

@article{jacobsModelBasedPurchasePredictions2016,
  title = {Model-{{Based Purchase Predictions}} for {{Large Assortments}}},
  author = {Jacobs, Bruno J.D. and Donkers, Bas and Fok, Dennis},
  year = {2016},
  month = may,
  journal = {Marketing Science},
  volume = {35},
  number = {3},
  pages = {389--404},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2016.0985},
  urldate = {2023-04-26},
  abstract = {An accurate prediction of what a customer will purchase next is of paramount importance to successful online retailing. In practice, customer purchase history data is readily available to make such predictions, sometimes complemented with customer characteristics. Given the large product assortments maintained by online retailers, scalability of the prediction method is just as important as its accuracy. We study two classes of models that use such data to predict what a customer will buy next, i.e., a novel approach that uses latent Dirichlet allocation (LDA), and mixtures of Dirichlet-Multinomials (MDM). A key benefit of a model-based approach is the potential to accommodate observed customer heterogeneity through the inclusion of predictor variables. We show that LDA can be extended in this direction while retaining its scalability. We apply the models to purchase data from an online retailer and contrast their predictive performance with that of a collaborative filter and a discrete choice model. Both LDA and MDM outperform the other methods. Moreover, LDA attains performance similar to that of MDM while being far more scalable, rendering it a promising approach to purchase prediction in large product assortments.             Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0985 .},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/LIEJ9IPZ/Jacobs et al. - 2016 - Model-Based Purchase Predictions for Large Assortm.pdf}
}

@article{jacobsonFormationExpectedFuture1990,
  title = {The {{Formation}} of {{Expected Future Price}}: {{A Reference Price}} for {{Forward- Looking Consumers}}},
  shorttitle = {The {{Formation}} of {{Expected Future Price}}},
  author = {Jacobson, Robert and Obermiller, Carl},
  year = {1990},
  month = mar,
  journal = {Journal of Consumer Research},
  volume = {16},
  number = {4},
  pages = {420},
  issn = {0093-5301, 1537-5277},
  doi = {10.1086/209227},
  urldate = {2023-07-09},
  langid = {english},
  keywords = {notion}
}

@article{jacobsTaleTailInference2021,
  title = {The {{Tale}} of the {{Tail}}: {{Inference}} for {{Customer Purchase Behavior}} in the {{Long Tail}}},
  shorttitle = {The {{Tale}} of the {{Tail}}},
  author = {Jacobs, Bruno and Nibbering, Didier},
  year = {2021},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3915874},
  urldate = {2023-04-26},
  abstract = {A large product assortment is typically characterized by many products that are rarely purchased: the long tail. Combined, these products make a sizable contribution to the total purchase volume. A retailer that better understands the purchase behavior in its long tail can increase the value of these products. Yet, analyzing tail purchases at the customer level is challenging: The available purchases per product in the tail are limited, while the number of customers and products are large. We develop new methodology that overcomes these challenges and sheds light on customer-specific purchase behavior for the long tail. The idea underlying our approach is a dimension reduction that uses latent product groups to summarize tail purchase behavior. We rely on variational inference to apply our method to a large-scale purchase history dataset with almost 50,000 products and over 3 million shopping trips. We are able to identify the customers that are likely to purchase in the tail of the assortment, how this varies across product categories, and how tail purchases relate to purchases of other products in the assortment. These insights can be used to improve recommendations of tail products, facilitate navigation through the assortment, and inform assortment management decisions.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/QCF49ABR/Jacobs and Nibbering - 2021 - The Tale of the Tail Inference for Customer Purch.pdf}
}

@article{jacobsUnderstandingLargeScaleDynamic2021,
  title = {Understanding {{Large-Scale Dynamic Purchase Behavior}}},
  author = {Jacobs, Bruno and Fok, Dennis and Donkers, Bas},
  year = {2021},
  month = sep,
  journal = {Marketing Science},
  volume = {40},
  number = {5},
  pages = {844--870},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2020.1279},
  urldate = {2023-04-26},
  abstract = {In modern retail contexts, retailers sell products from vast product assortments to a large and heterogeneous customer base. Understanding purchase behavior in such a context is very important. Standard models cannot be used because of the high dimensionality of the data. We propose a new model that creates an efficient dimension reduction through the idea of purchase motivations. We only require customer-level purchase history data, which is ubiquitous in modern retailing. The model handles large-scale data and even works in settings with shopping trips consisting of few purchases. Essential features of our model are that it accounts for the product, customer, and time dimensions present in purchase history data; relates the relevance of motivations to customer- and shopping-trip characteristics; captures interdependencies between motivations; and achieves superior predictive performance. Estimation results from this comprehensive model provide deep insights into purchase behavior. Such insights can be used by managers to create more intuitive, better informed, and more effective marketing actions. As scalability of the model is essential for practical applicability, we develop a fast, custom-made inference algorithm based on variational inference. We illustrate the model using purchase history data from a Fortune 500 retailer involving more than 4,000 unique products.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/QYF53YR3/Jacobs et al. - 2021 - Understanding Large-Scale Dynamic Purchase Behavio.pdf}
}

@misc{jinCanLargeLanguage2023,
  title = {Can {{Large Language Models Infer Causation}} from {{Correlation}}?},
  author = {Jin, Zhijing and Liu, Jiarui and Lyu, Zhiheng and Poff, Spencer and Sachan, Mrinmaya and Mihalcea, Rada and Diab, Mona and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05836},
  eprint = {2306.05836},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/hoener/Zotero/storage/MR5B8UF7/Jin et al. - 2023 - Can Large Language Models Infer Causation from Cor.pdf;/Users/hoener/Zotero/storage/76VY8SGV/2306.html}
}

@article{jinRACRecReviewAware2020,
  title = {{{RACRec}}: {{Review Aware Cross-Domain Recommendation}} for {{Fully-Cold-Start User}}},
  shorttitle = {{{RACRec}}},
  author = {Jin, Yaru and Dong, Shoubin and Cai, Yong and Hu, Jinlong},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {55032--55041},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2982037},
  urldate = {2023-07-09},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/PT7HB2EN/Jin et al. - 2020 - RACRec Review Aware Cross-Domain Recommendation f.pdf}
}

@book{kardes2014consumer,
  title = {Consumer Behavior},
  author = {Kardes, Frank and Cronley, Maria and Cline, Thomas},
  year = {2014},
  publisher = {Cengage Learning},
  keywords = {notion}
}

@article{kimModelingDynamicsCrowdfunding2020,
  title = {Modeling {{Dynamics}} in {{Crowdfunding}}},
  author = {Kim, Chul and Kannan, P. K. and Trusov, Michael and Ordanini, Andrea},
  year = {2020},
  month = mar,
  journal = {Marketing Science},
  volume = {39},
  number = {2},
  pages = {339--365},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2019.1209},
  urldate = {2023-10-12},
  abstract = {This paper investigates the underlying mechanisms of crowdfunding behavior (forward-looking delaying investment behavior and social interactions), which lead to the crowdfunding dynamics.           ,              We investigate various dynamics characterizing the crowdfunding process: stagnation after friend-funding, gradual increase through crowd participation, and acceleration in the last phase. We propose three mechanisms as major drivers of the crowdfunding dynamics: forward-looking delaying investment behavior, contemporaneous social interactions, and forward-looking social interactions. We apply the rational expectations equilibrium of the approximate aggregation approach to model the underlying mechanisms. Using the Bayesian IJC method, we analyze individual-level investment data from a crowdfunding platform, Sellaband. We find strong evidence for the three mechanisms and confirm that they contribute to the contrasting dynamic patterns observed in our data. We also simulate counterfactuals to derive optimal policy decisions for both fundraisers and platforms. For fundraisers, we infer the optimal goals that ensure goal completion while raising the maximum capital. For platforms, we suggest an optimal targeting strategy that identifies those crowdfunders who contribute the most to the crowding process and, ultimately, goal success. Also, we provide critical input for various resource allocation decisions by accurately predicting whether the project will succeed and when it will succeed at the time when 50\% of the goal has been achieved.},
  langid = {english},
  keywords = {notion}
}

@article{kochDynamicCustomerJourney2023,
  title = {Dynamic Customer Journey Analysis and Its Advertising Impact},
  author = {Koch, Christian and Lindenbeck, Benedikt and Olbrich, Rainer},
  year = {2023},
  month = jan,
  journal = {Journal of Strategic Marketing},
  pages = {1--20},
  issn = {0965-254X, 1466-4488},
  doi = {10.1080/0965254X.2023.2171475},
  urldate = {2023-09-26},
  langid = {english},
  keywords = {notion}
}

@article{koehnPredictingOnlineShopping2020,
  title = {Predicting Online Shopping Behaviour from Clickstream Data Using Deep Learning},
  author = {Koehn, Dennis and Lessmann, Stefan and Schaal, Markus},
  year = {2020},
  month = jul,
  journal = {Expert Systems with Applications},
  volume = {150},
  pages = {113342},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113342},
  urldate = {2023-05-09},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/XMNITQLS/Koehn et al. - 2020 - Predicting online shopping behaviour from clickstr.pdf}
}

@article{kolaySlottingFeesPrice2022,
  title = {Slotting {{Fees}} and {{Price Discrimination}} in {{Retail Channels}}},
  author = {Kolay, Sreya and Shaffer, Greg},
  year = {2022},
  month = nov,
  journal = {Marketing Science},
  volume = {41},
  number = {6},
  pages = {1145--1162},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2022.1373},
  urldate = {2023-10-09},
  abstract = {This paper studies how slotting fees can be used by a manufacturer to discriminate indirectly between retailers when overt discrimination is infeasible.           ,              We consider a manufacturer's optimal contract design in the presence of asymmetric retailers. We first solve for the case in which the manufacturer can overtly discriminate between its retailers by offering each a different two-part tariff contract. We then compare it to the case in which overt discrimination is not feasible and the manufacturer must instead induce self-selection. We find that whereas there is no role for slotting fees when overt discrimination is possible, slotting fees can arise as part of the equilibrium menu of contracts offered to both retailers in the absence of such discrimination. It is found that the large retailer always prefers the contract with the lower wholesale price, even if it means not accepting a slotting fee, whereas the small retailer always accepts the largest slotting fee offered, even though it means taking on the contract with the higher wholesale price. Our results also suggest that the wholesale prices (and hence retail prices) of both retailers will be uniformly higher when overt discrimination is infeasible.             History: Ganesh Iyer served as the senior editor and Debu Purohit served as the associate editor for this article.             Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2022.1373 .},
  langid = {english},
  keywords = {notion}
}

@book{korff2021competition,
  title = {Competition on the Fast Lane: {{The}} Price Structure of Homogeneous Retail Gasoline Stations},
  author = {Korff, Alex},
  year = {2021},
  number = {359},
  publisher = {DICE Discussion Paper},
  keywords = {notion}
}

@article{lemonUnderstandingCustomerExperience2016,
  title = {Understanding {{Customer Experience Throughout}} the {{Customer Journey}}},
  author = {Lemon, Katherine N. and Verhoef, Peter C.},
  year = {2016},
  month = nov,
  journal = {Journal of Marketing},
  volume = {80},
  number = {6},
  pages = {69--96},
  issn = {0022-2429, 1547-7185},
  doi = {10.1509/jm.15.0420},
  urldate = {2023-07-10},
  abstract = {Understanding customer experience and the customer journey over time is critical for firms. Customers now interact with firms through myriad touch points in multiple channels and media, and customer experiences are more social in nature. These changes require firms to integrate multiple business functions, and even external partners, in creating and delivering positive customer experiences. In this article, the authors aim to develop a stronger understanding of customer experience and the customer journey in this era of increasingly complex customer behavior. To achieve this goal, they examine existing definitions and conceptualizations of customer experience as a construct and provide a historical perspective of the roots of customer experience within marketing. Next, they attempt to bring together what is currently known about customer experience, customer journeys, and customer experience management. Finally, they identify critical areas for future research on this important topic.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/KL776LWA/Lemon and Verhoef - 2016 - Understanding Customer Experience Throughout the C.pdf}
}

@article{lemonUnderstandingCustomerExperience2016a,
  title = {Understanding {{Customer Experience Throughout}} the {{Customer Journey}}},
  author = {Lemon, Katherine N. and Verhoef, Peter C.},
  year = {2016},
  month = nov,
  journal = {Journal of Marketing},
  volume = {80},
  number = {6},
  pages = {69--96},
  issn = {0022-2429, 1547-7185},
  doi = {10.1509/jm.15.0420},
  urldate = {2023-09-27},
  abstract = {Understanding customer experience and the customer journey over time is critical for firms. Customers now interact with firms through myriad touch points in multiple channels and media, and customer experiences are more social in nature. These changes require firms to integrate multiple business functions, and even external partners, in creating and delivering positive customer experiences. In this article, the authors aim to develop a stronger understanding of customer experience and the customer journey in this era of increasingly complex customer behavior. To achieve this goal, they examine existing definitions and conceptualizations of customer experience as a construct and provide a historical perspective of the roots of customer experience within marketing. Next, they attempt to bring together what is currently known about customer experience, customer journeys, and customer experience management. Finally, they identify critical areas for future research on this important topic.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/REXZ6F4H/Lemon and Verhoef - 2016 - Understanding Customer Experience Throughout the C.pdf}
}

@article{limaEquityPremiumPrediction2023,
  title = {Equity-premium Prediction: {{Attention}} Is All You Need},
  shorttitle = {Equity-premium Prediction},
  author = {Lima, Luiz Renato and Godeiro, Lucas L{\'u}cio},
  year = {2023},
  month = jan,
  journal = {Journal of Applied Econometrics},
  volume = {38},
  number = {1},
  pages = {105--122},
  issn = {0883-7252, 1099-1255},
  doi = {10.1002/jae.2939},
  urldate = {2023-10-09},
  abstract = {Summary             Predictions of stock returns are greatly improved relative to low-dimensional forecasting regressions when the forecasts are based on the estimated factor of large data sets, also known as the diffusion index (DI) model. However, when applied to text data, DI models do not perform well. This paper shows that by simply using text data in a DI model does not improve equity-premium forecasts over the naive historical-average model, but substantial gains are obtained when one selects the most predictive words before computing the factors and allows the dictionary to be updated over time.},
  langid = {english},
  keywords = {notion}
}

@article{limOperatorvaluedKernelbasedVector2015,
  title = {Operator-Valued Kernel-Based Vector Autoregressive Models for Network Inference},
  author = {Lim, N{\'e}h{\'e}my and {d'Alch{\'e}-Buc}, Florence and Auliac, C{\'e}dric and Michailidis, George},
  year = {2015},
  month = jun,
  journal = {Machine Learning},
  volume = {99},
  number = {3},
  pages = {489--513},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-014-5479-3},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/XLNEKNKR/Lim et al. - 2015 - Operator-valued kernel-based vector autoregressive.pdf}
}

@article{lindqvistNovelMethodFunction2022,
  title = {A {{Novel Method}} for {{Function Smoothness}} in {{Neural Networks}}},
  author = {Lindqvist, Blerta},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {75354--75364},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3189363},
  urldate = {2023-10-17},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/9YCLNT5A/Lindqvist - 2022 - A Novel Method for Function Smoothness in Neural N.pdf}
}

@article{linDualContrastiveNetwork2022,
  title = {Dual {{Contrastive Network}} for {{Sequential Recommendation}} with {{User}} and {{Item-Centric Perspectives}}},
  author = {Lin, Guanyu and Gao, Chen and Li, Yinfeng and Zheng, Yu and Li, Zhiheng and Jin, Depeng and Li, Dong and Hao, Jianye and Li, Yong},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2209.08446},
  urldate = {2023-05-09},
  abstract = {With the outbreak of today's streaming data, the sequential recommendation is a promising solution to achieve time-aware personalized modeling. It aims to infer the next interacted item of a given user based on the history item sequence. Some recent works tend to improve the sequential recommendation via random masking on the history item so as to generate self-supervised signals. But such approaches will indeed result in sparser item sequence and unreliable signals. Besides, the existing sequential recommendation models are only user-centric, i.e., based on the historical items by chronological order to predict the probability of candidate items, which ignores whether the items from a provider can be successfully recommended. Such user-centric recommendation will make it impossible for the provider to expose their new items and result in popular bias. In this paper, we propose a novel Dual Contrastive Network (DCN) to generate ground-truth self-supervised signals for sequential recommendation by auxiliary user-sequence from an item-centric perspective. Specifically, we propose dual representation contrastive learning to refine the representation learning by minimizing the Euclidean distance between the representations of a given user/item and history items/users of them. Before the second contrastive learning module, we perform the next user prediction to capture the trends of items preferred by certain types of users and provide personalized exploration opportunities for item providers. Finally, we further propose dual interest contrastive learning to self-supervise the dynamic interest from the next item/user prediction and static interest of matching probability. Experiments on four benchmark datasets verify the effectiveness of our proposed method. Further ablation study also illustrates the boosting effect of the proposed components upon different sequential models.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,Information Retrieval (cs.IR),notion}
}

@article{liTimeawareHyperbolicGraph2023,
  title = {Time-Aware {{Hyperbolic Graph Attention Network}} for {{Session-based Recommendation}}},
  author = {Li, Xiaohan and Liu, Yuqing and Liu, Zheng and Yu, Philip S.},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.03780},
  urldate = {2023-05-09},
  abstract = {Session-based Recommendation (SBR) is to predict users' next interested items based on their previous browsing sessions. Existing methods model sessions as graphs or sequences to estimate user interests based on their interacted items to make recommendations. In recent years, graph-based methods have achieved outstanding performance on SBR. However, none of these methods consider temporal information, which is a crucial feature in SBR as it indicates timeliness or currency. Besides, the session graphs exhibit a hierarchical structure and are demonstrated to be suitable in hyperbolic geometry. But few papers design the models in hyperbolic spaces and this direction is still under exploration. In this paper, we propose Time-aware Hyperbolic Graph Attention Network (TA-HGAT) - a novel hyperbolic graph neural network framework to build a session-based recommendation model considering temporal information. More specifically, there are three components in TA-HGAT. First, a hyperbolic projection module transforms the item features into hyperbolic space. Second, the time-aware graph attention module models time intervals between items and the users' current interests. Third, an evolutionary loss at the end of the model provides an accurate prediction of the recommended item based on the given timestamp. TA-HGAT is built in a hyperbolic space to learn the hierarchical structure of session graphs. Experimental results show that the proposed TA-HGAT has the best performance compared to ten baseline models on two real-world datasets.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Information Retrieval (cs.IR),Machine Learning (cs.LG),notion,Social and Information Networks (cs.SI)}
}

@inproceedings{liuBasketRecommendationMultiIntent2020,
  title = {Basket {{Recommendation}} with {{Multi-Intent Translation Graph Neural Network}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Liu, Zhiwei and Li, Xiaohan and Fan, Ziwei and Guo, Stephen and Achan, Kannan and Yu, Philip S.},
  year = {2020},
  month = dec,
  pages = {728--737},
  publisher = {IEEE},
  address = {Atlanta, GA, USA},
  doi = {10.1109/BigData50022.2020.9377917},
  urldate = {2023-05-09},
  isbn = {978-1-72816-251-5},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/EGNWAQVS/Liu et al. - 2020 - Basket Recommendation with Multi-Intent Translatio.pdf}
}

@article{liuBasketRecommendationMultiIntent2020a,
  title = {Basket {{Recommendation}} with {{Multi-Intent Translation Graph Neural Network}}},
  author = {Liu, Zhiwei and Li, Xiaohan and Fan, Ziwei and Guo, Stephen and Achan, Kannan and Yu, Philip S.},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2010.11419},
  urldate = {2023-05-09},
  abstract = {The problem of basket recommendation{\textasciitilde}(BR) is to recommend a ranking list of items to the current basket. Existing methods solve this problem by assuming the items within the same basket are correlated by one semantic relation, thus optimizing the item embeddings. However, this assumption breaks when there exist multiple intents within a basket. For example, assuming a basket contains {\textbackslash}\{{\textbackslash}textit\{bread, cereal, yogurt, soap, detergent\}{\textbackslash}\} where {\textbackslash}\{{\textbackslash}textit\{bread, cereal, yogurt\}{\textbackslash}\} are correlated through the "breakfast" intent, while {\textbackslash}\{{\textbackslash}textit\{soap, detergent\}{\textbackslash}\} are of "cleaning" intent, ignoring multiple relations among the items spoils the ability of the model to learn the embeddings. To resolve this issue, it is required to discover the intents within the basket. However, retrieving a multi-intent pattern is rather challenging, as intents are latent within the basket. Additionally, intents within the basket may also be correlated. Moreover, discovering a multi-intent pattern requires modeling high-order interactions, as the intents across different baskets are also correlated. To this end, we propose a new framework named as {\textbackslash}textbf\{M\}ulti-{\textbackslash}textbf\{I\}ntent {\textbackslash}textbf\{T\}ranslation {\textbackslash}textbf\{G\}raph {\textbackslash}textbf\{N\}eural {\textbackslash}textbf\{N\}etwork{\textasciitilde}(\{{\textbackslash}textbf\{MITGNN\}\}). MITGNN models \$T\$ intents as tail entities translated from one corresponding basket embedding via \$T\$ relation vectors. The relation vectors are learned through multi-head aggregators to handle user and item information. Additionally, MITGNN propagates multiple intents across our defined basket graph to learn the embeddings of users and items by aggregating neighbors. Extensive experiments on two real-world datasets prove the effectiveness of our proposed model on both transductive and inductive BR. The code is available online at https://github.com/JimLiu96/MITGNN.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Information Retrieval (cs.IR),notion}
}

@inproceedings{liuLearningSmoothNeural2022,
  title = {Learning {{Smooth Neural Functions}} via {{Lipschitz Regularization}}},
  booktitle = {Special {{Interest Group}} on {{Computer Graphics}} and {{Interactive Techniques Conference Proceedings}}},
  author = {Liu, Hsueh-Ti Derek and Williams, Francis and Jacobson, Alec and Fidler, Sanja and Litany, Or},
  year = {2022},
  month = aug,
  pages = {1--13},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3528233.3530713},
  urldate = {2023-10-17},
  isbn = {978-1-4503-9337-9},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/XQYQAJ4C/Liu et al. - 2022 - Learning Smooth Neural Functions via Lipschitz Reg.pdf}
}

@article{liuMultiModalFakeNews2023,
  title = {Multi-{{Modal Fake News Detection}} via {{Bridging}} the {{Gap}} between {{Modals}}},
  author = {Liu, Peng and Qian, Wenhua and Xu, Dan and Ren, Bingling and Cao, Jinde},
  year = {2023},
  month = apr,
  journal = {Entropy},
  volume = {25},
  number = {4},
  pages = {614},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e25040614},
  urldate = {2024-02-25},
  abstract = {Multi-modal fake news detection aims to identify fake information through text and corresponding images. The current methods purely combine images and text scenarios by a vanilla attention module but there exists a semantic gap between different scenarios. To address this issue, we introduce an image caption-based method to enhance the model's ability to capture semantic information from images. Formally, we integrate image description information into the text to bridge the semantic gap between text and images. Moreover, to optimize image utilization and enhance the semantic interaction between images and text, we combine global and object features from the images for the final representation. Finally, we leverage a transformer to fuse the above multi-modal content. We carried out extensive experiments on two publicly available datasets, and the results show that our proposed method significantly improves performance compared to other existing methods.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {caption-based,fake news detection,multi-modal,notion,transformer},
  file = {/Users/hoener/Zotero/storage/WG5IKDV5/Liu et al. - 2023 - Multi-Modal Fake News Detection via Bridging the G.pdf}
}

@article{louposWhatReviewsForetell2023,
  title = {What Reviews Foretell about Opening Weekend Box Office Revenue: The Harbinger of Failure Effect in the Movie Industry},
  shorttitle = {What Reviews Foretell about Opening Weekend Box Office Revenue},
  author = {Loupos, Pantelis and Peng, Yvette and Li, Sute and Hao, Hao},
  year = {2023},
  month = sep,
  journal = {Marketing Letters},
  volume = {34},
  number = {3},
  pages = {513--534},
  issn = {0923-0645, 1573-059X},
  doi = {10.1007/s11002-023-09665-8},
  urldate = {2023-10-09},
  abstract = {Abstract             We empirically investigate the harbinger of failure phenomenon in the motion picture industry by analyzing the pre-release reviews written on movies by film critics. We find that harbingers of failure do exist. Their positive (negative) pre-release movie reviews provide a strong predictive signal that the movie will turn out to be a flop (success). This signal persists even for the top critic category, which usually consists of professional critics, indicating that having expertise in a professional domain does not necessarily lead to correct predictions. Our findings challenge the current belief that positive reviews always help enhance box office revenue and shed new light on the influencer-predictor hypothesis. We further analyze the writing style of harbingers and provide new insights into their personality traits and cognitive biases.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/ZRHA238P/Loupos et al. - 2023 - What reviews foretell about opening weekend box of.pdf}
}

@article{luchsYesThisOther2017,
  title = {``{{Yes}}, but This {{Other One Looks Better}}/{{Works Better}}'': {{How}} Do {{Consumers Respond}} to {{Trade-offs Between Sustainability}} and {{Other Valued Attributes}}?},
  shorttitle = {``{{Yes}}, but This {{Other One Looks Better}}/{{Works Better}}''},
  author = {Luchs, Michael G. and Kumar, Minu},
  year = {2017},
  month = feb,
  journal = {Journal of Business Ethics},
  volume = {140},
  number = {3},
  pages = {567--584},
  issn = {0167-4544, 1573-0697},
  doi = {10.1007/s10551-015-2695-0},
  urldate = {2023-06-26},
  langid = {english},
  keywords = {notion}
}

@techreport{ludwigMachineLearningTool2023,
  title = {Machine {{Learning}} as a {{Tool}} for {{Hypothesis Generation}}},
  author = {Ludwig, Jens and Mullainathan, Sendhil},
  year = {2023},
  month = mar,
  number = {w31017},
  pages = {w31017},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w31017},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/YHISPA8M/Ludwig and Mullainathan - 2023 - Machine Learning as a Tool for Hypothesis Generati.pdf}
}

@misc{mariani2020wisdom,
  title = {The Wisdom of the Few: {{Predicting}} Collective Success from Individual Behavior},
  author = {Mariani, Manuel S. and Gimenez, Yanina and Brea, Jorge and Minnoni, Martin and Algesheimer, Ren{\'e} and Tessone, Claudio J.},
  year = {2020},
  eprint = {2001.04777},
  primaryclass = {physics.soc-ph},
  archiveprefix = {arxiv},
  keywords = {notion}
}

@misc{marianiWisdomFewPredicting2020,
  title = {The Wisdom of the Few: {{Predicting}} Collective Success from Individual Behavior},
  shorttitle = {The Wisdom of the Few},
  author = {Mariani, Manuel S. and Gimenez, Yanina and Brea, Jorge and Minnoni, Martin and Algesheimer, Ren{\'e} and Tessone, Claudio J.},
  year = {2020},
  month = jun,
  number = {arXiv:2001.04777},
  eprint = {2001.04777},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2023-10-09},
  abstract = {Can we predict top-performing products, services, or businesses by only monitoring the behavior of a small set of individuals? Although most previous studies focused on the predictive power of "hub" individuals with many social contacts, which sources of customer behavioral data are needed to address this question remains unclear, mostly due to the scarcity of available datasets that simultaneously capture individuals' purchasing patterns and social interactions. Here, we address this question in a unique, large-scale dataset that combines individuals' credit-card purchasing history with their social and mobility traits across an entire nation. Surprisingly, we find that the purchasing history alone enables the detection of small sets of ``discoverers" whose early purchases offer reliable success predictions for the brick-and-mortar stores they visit. In contrast with the assumptions by most existing studies on word-of-mouth processes, the hubs selected by social network centrality are not consistently predictive of success. Our findings show that companies and organizations with access to large-scale purchasing data can detect the discoverers and leverage their behavior to anticipate market trends, without the need for social network data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks,notion,Physics - Physics and Society},
  file = {/Users/hoener/Zotero/storage/H5AMRBVG/Mariani et al. - 2020 - The wisdom of the few Predicting collective succe.pdf;/Users/hoener/Zotero/storage/Z2C7DZYT/2001.html}
}

@article{mcfadden1981econometric,
  title = {Econometric Models of Probabilistic Choice},
  author = {McFadden, Daniel},
  year = {1981},
  journal = {Structural analysis of discrete data with econometric applications},
  volume = {198272},
  publisher = {MIT Press},
  keywords = {notion}
}

@article{mcfadden1986choice,
  title = {The Choice Theory Approach to Market Research},
  author = {McFadden, Daniel},
  year = {1986},
  journal = {Marketing science},
  volume = {5},
  number = {4},
  pages = {275--297},
  publisher = {INFORMS},
  keywords = {notion}
}

@misc{MegaSynIntegratingGenerative,
  title = {{{MegaSyn}}: {{Integrating Generative Molecular Design}}, {{Automated Analog Designer}}, and {{Synthetic Viability Prediction}} {\textbar} {{ACS Omega}}},
  urldate = {2024-04-07},
  howpublished = {https://pubs.acs.org/doi/full/10.1021/acsomega.2c01404},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/TNFMQTH7/acsomega.html}
}

@article{meyersNovoMolecularDesign2021,
  title = {{\emph{De Novo}} Molecular Design and Generative Models},
  author = {Meyers, Joshua and Fabian, Benedek and Brown, Nathan},
  year = {2021},
  month = nov,
  journal = {Drug Discovery Today},
  volume = {26},
  number = {11},
  pages = {2707--2715},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2021.05.019},
  urldate = {2024-04-07},
  abstract = {Molecular design strategies are integral to therapeutic progress in drug discovery. Computational approaches for de novo molecular design have been developed over the past three decades and, recently, thanks in part to advances in machine learning (ML) and artificial intelligence (AI), the drug discovery field has gained practical experience. Here, we review these learnings and present de novo approaches according to the coarseness of their molecular representation: that is, whether molecular design is modeled on an atom-based, fragment-based, or reaction-based paradigm. Furthermore, we emphasize the value of strong benchmarks, describe the main challenges to using these methods in practice, and provide a viewpoint on further opportunities for exploration and challenges to be tackled in the upcoming years.},
  keywords = {Artificial intelligence,Atom-based,Automated design,design,Fragment-based,Generative chemistry,Generative models,Molecular design,Molecular representation,notion,Reaction-based},
  file = {/Users/hoener/Zotero/storage/ZBPFXJ2I/S1359644621002531.html}
}

@article{mikolov2013distributed,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  journal = {Advances in neural information processing systems},
  volume = {26},
  keywords = {notion}
}

@article{montgomeryProspectsPersonalizationInternet2009,
  title = {Prospects for {{Personalization}} on the {{Internet}}},
  author = {Montgomery, Alan L. and Smith, Michael D.},
  year = {2009},
  month = may,
  journal = {Journal of Interactive Marketing},
  volume = {23},
  number = {2},
  pages = {130--137},
  issn = {1094-9968, 1520-6653},
  doi = {10.1016/j.intmar.2009.02.001},
  urldate = {2023-06-25},
  abstract = {Personalization is a key component of an interactive marketing strategy. Its purpose is to adapt a standardized product or service to an individual customer's needs. The goal is to create profit for the producer and increased value for the consumer. This goal fits nicely into traditional notions of segmentation. Applications of personalization have advanced greatly in conjunction with the Internet, since it provides an environment that is information rich and well suited to interactivity. This article reviews past research on personalization and considers some examples of personalization in practice. We discuss what we believe are key problems and directions for personalization in the future.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/23AF9NAQ/Montgomery and Smith - 2009 - Prospects for Personalization on the Internet.pdf}
}

@article{mullainathanPredictiveAlgorithmsAutomatic2023,
  title = {From {{Predictive Algorithms}} to {{Automatic Generation}} of {{Anomalies}}},
  author = {Mullainathan, Sendhil and Rambachan, Ashesh},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4443738},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion}
}

@misc{MultimodalFakeNews,
  title = {Multimodal {{Fake News Analysis Based}} on {{Image}}--{{Text Similarity}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-02-25},
  howpublished = {https://ieeexplore-ieee-org.vu-nl.idm.oclc.org/document/10049384},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/UXS99E2C/10049384.html}
}

@misc{ngPredictingOutofDomainGeneralization2023,
  title = {Predicting {{Out-of-Domain Generalization}} with {{Neighborhood Invariance}}},
  author = {Ng, Nathan and Hulkund, Neha and Cho, Kyunghyun and Ghassemi, Marzyeh},
  year = {2023},
  month = jul,
  number = {arXiv:2207.02093},
  eprint = {2207.02093},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-17},
  abstract = {Developing and deploying machine learning models safely depends on the ability to characterize and compare their abilities to generalize to new environments. Although recent work has proposed a variety of methods that can directly predict or theoretically bound the generalization capacity of a model, they rely on strong assumptions such as matching train/test distributions and access to model gradients. In order to characterize generalization when these assumptions are not satisfied, we propose neighborhood invariance, a measure of a classifier's output invariance in a local transformation neighborhood. Specifically, we sample a set of transformations and given an input test point, calculate the invariance as the largest fraction of transformed points classified into the same class. Crucially, our measure is simple to calculate, does not depend on the test point's true label, makes no assumptions about the data distribution or model, and can be applied even in out-of-domain (OOD) settings where existing methods cannot, requiring only selecting a set of appropriate data transformations. In experiments on robustness benchmarks in image classification, sentiment analysis, and natural language inference, we demonstrate a strong and robust correlation between our neighborhood invariance measure and actual OOD generalization on over 4,600 models evaluated on over 100 unique train/test domain pairs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/hoener/Zotero/storage/X45WSC8B/Ng et al. - 2023 - Predicting Out-of-Domain Generalization with Neigh.pdf;/Users/hoener/Zotero/storage/YGRKV779/2207.html}
}

@inproceedings{NIPS2017_51e6d6e6,
  title = {A Meta-Learning Perspective on Cold-Start Recommendations for Items},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vartak, Manasi and Thiagarajan, Arvind and Miranda, Conrado and Bratman, Jeshua and Larochelle, Hugo},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  keywords = {notion}
}

@inproceedings{nNeuralAttentionBased2018,
  title = {A Neural Attention Based Approach for Clickstream Mining},
  booktitle = {Proceedings of the {{ACM India Joint International Conference}} on {{Data Science}} and {{Management}} of {{Data}}},
  author = {N, Chandramohan T and Ravindran, Balaraman},
  year = {2018},
  month = jan,
  pages = {118--127},
  publisher = {ACM},
  address = {Goa India},
  doi = {10.1145/3152494.3152505},
  urldate = {2023-09-26},
  isbn = {978-1-4503-6341-9},
  langid = {english},
  keywords = {notion}
}

@article{padillaOvercomingColdStart2021,
  title = {Overcoming the {{Cold Start Problem}} of {{Customer Relationship Management Using}} a {{Probabilistic Machine Learning Approach}}},
  author = {Padilla, Nicolas and Ascarza, Eva},
  year = {2021},
  month = oct,
  journal = {Journal of Marketing Research},
  volume = {58},
  number = {5},
  pages = {981--1006},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/00222437211032938},
  urldate = {2023-09-26},
  abstract = {The success of customer relationship management programs ultimately depends on the firm's ability to identify and leverage differences across customers---a difficult task when firms attempt to manage new customers, for whom only the first purchase has been observed. The lack of repeated observations for these customers poses a structural challenge for firms to infer unobserved differences across them. This is what the authors call the ``cold start'' problem of customer relationship management, whereby companies have difficulties leveraging existing data when they attempt to make inferences about customers at the beginning of their relationship. The authors propose a solution to the cold start problem by developing a probabilistic machine learning modeling framework that leverages the information collected at the moment of acquisition. The main aspect of the model is that it flexibly captures latent dimensions that govern the behaviors observed at acquisition as well as future propensities to buy and to respond to marketing actions using deep exponential families. The model can be integrated with a variety of demand specifications and is flexible enough to capture a wide range of heterogeneity structures. The authors validate their approach in a retail context and empirically demonstrate the model's ability to identify high-value customers as well as those most sensitive to marketing actions right after their first purchase.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/CJ78A48R/Padilla and Ascarza - 2021 - Overcoming the Cold Start Problem of Customer Rela.pdf}
}

@article{padillaOvercomingColdStart2021a,
  title = {Overcoming the {{Cold Start Problem}} of {{Customer Relationship Management Using}} a {{Probabilistic Machine Learning Approach}}},
  author = {Padilla, Nicolas and Ascarza, Eva},
  year = {2021},
  month = oct,
  journal = {Journal of Marketing Research},
  volume = {58},
  number = {5},
  pages = {981--1006},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/00222437211032938},
  urldate = {2023-09-27},
  abstract = {The success of customer relationship management programs ultimately depends on the firm's ability to identify and leverage differences across customers---a difficult task when firms attempt to manage new customers, for whom only the first purchase has been observed. The lack of repeated observations for these customers poses a structural challenge for firms to infer unobserved differences across them. This is what the authors call the ``cold start'' problem of customer relationship management, whereby companies have difficulties leveraging existing data when they attempt to make inferences about customers at the beginning of their relationship. The authors propose a solution to the cold start problem by developing a probabilistic machine learning modeling framework that leverages the information collected at the moment of acquisition. The main aspect of the model is that it flexibly captures latent dimensions that govern the behaviors observed at acquisition as well as future propensities to buy and to respond to marketing actions using deep exponential families. The model can be integrated with a variety of demand specifications and is flexible enough to capture a wide range of heterogeneity structures. The authors validate their approach in a retail context and empirically demonstrate the model's ability to identify high-value customers as well as those most sensitive to marketing actions right after their first purchase.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/HRXYTYUC/Padilla and Ascarza - 2021 - Overcoming the Cold Start Problem of Customer Rela.pdf}
}

@article{pangDeepGenerativeModels2023,
  title = {Deep {{Generative Models}} in {{De Novo Drug Molecule Generation}}},
  author = {Pang, Chao and Qiao, Jianbo and Zeng, Xiangxiang and Zou, Quan and Wei, Leyi},
  year = {2023},
  month = nov,
  journal = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.3c01496},
  urldate = {2024-04-07},
  abstract = {The discovery of new drugs has important implications for human health. Traditional methods for drug discovery rely on experiments to optimize the structure of lead molecules, which are time-consuming and high-cost. Recently, artificial intelligence has exhibited promising and efficient performance for drug-like molecule generation. In particular, deep generative models achieve great success in de novo generation of drug-like molecules with desired properties, showing massive potential for novel drug discovery. In this study, we review the recent progress of molecule generation using deep generative models, mainly focusing on molecule representations, public databases, data processing tools, and advanced artificial intelligence based molecule generation frameworks. In particular, we present a comprehensive comparison of state-of-the-art deep generative models for molecule generation and a summary of commonly used molecular design strategies. We identify research gaps and challenges of molecule generation such as the need for better databases, missing 3D information in molecular representation, and the lack of high-precision evaluation metrics. We suggest future directions for molecular generation and drug discovery.},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/WGXXSX8J/Pang et al. - 2023 - Deep Generative Models in De Novo Drug Molecule Ge.pdf}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2024-02-28},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/6ETLLAZL/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{pmlr-v202-wang23v,
  title = {Direct Parameterization of {{Lipschitz-Bounded}} Deep Networks},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  author = {Wang, Ruigang and Manchester, Ian},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  year = {2023-07-23/2023-07-29},
  series = {Proceedings of Machine Learning Research},
  volume = {202},
  pages = {36093--36110},
  publisher = {PMLR},
  abstract = {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed {$^2$} Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP). We provide a ``direct'' parameterization, i.e., a smooth mapping from R\textsuperscript{N} onto the set of weights satisfying the SDP-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the SDP bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the SDP constraint. The new parameterization can equivalently be thought of as either a new layer type (the {\textexclamdown}em{\textquestiondown}sandwich layer{\textexclamdown}/em{\textquestiondown}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. A comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on both empirical and certified robust accuracy. Code is available at https://github.com/acfr/LBDN.},
  keywords = {notion}
}

@article{radford2018improving,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year = {2018},
  publisher = {OpenAI},
  keywords = {notion}
}

@article{ratner1999choosing,
  title = {Choosing Less-Preferred Experiences for the Sake of Variety},
  author = {Ratner, Rebecca K and Kahn, Barbara E and Kahneman, Daniel},
  year = {1999},
  journal = {Journal of consumer research},
  volume = {26},
  number = {1},
  pages = {1--15},
  publisher = {The University of Chicago Press},
  keywords = {notion}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1908.10084},
  urldate = {2023-07-09},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,notion}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019a,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-28},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,notion},
  file = {/Users/hoener/Zotero/storage/PNI3J3P7/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/hoener/Zotero/storage/RYQKWFAX/1908.html}
}

@inproceedings{rendleFactorizingPersonalizedMarkov2010,
  title = {Factorizing Personalized {{Markov}} Chains for Next-Basket Recommendation},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Rendle, Steffen and Freudenthaler, Christoph and {Schmidt-Thieme}, Lars},
  year = {2010},
  month = apr,
  pages = {811--820},
  publisher = {ACM},
  address = {Raleigh North Carolina USA},
  doi = {10.1145/1772690.1772773},
  urldate = {2023-05-09},
  isbn = {978-1-60558-799-8},
  langid = {english},
  keywords = {notion}
}

@misc{rojas-carullaInvariantModelsCausal2018,
  title = {Invariant {{Models}} for {{Causal Transfer Learning}}},
  author = {{Rojas-Carulla}, Mateo and Sch{\"o}lkopf, Bernhard and Turner, Richard and Peters, Jonas},
  year = {2018},
  month = sep,
  number = {arXiv:1507.05333},
  eprint = {1507.05333},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
  archiveprefix = {arxiv},
  keywords = {notion,Statistics - Machine Learning},
  file = {/Users/hoener/Zotero/storage/337CVBNT/Rojas-Carulla et al. - 2018 - Invariant Models for Causal Transfer Learning.pdf;/Users/hoener/Zotero/storage/KHTF573T/1507.html}
}

@article{rosca2020case,
  title = {A Case for New Neural Network Smoothness Constraints},
  author = {Rosca, Mihaela and Weber, Theophane and Gretton, Arthur and Mohamed, Shakir},
  year = {2020},
  publisher = {PMLR},
  keywords = {notion}
}

@article{rossi2021measuring,
  title = {Measuring Competition for Attention in Social Media: {{National}} Women's Soccer League Players on {{Twitter}}},
  author = {Rossi, Federico and Rubera, Gaia},
  year = {2021},
  journal = {Marketing Science},
  volume = {40},
  number = {6},
  pages = {1147--1168},
  publisher = {INFORMS},
  keywords = {notion}
}

@misc{ruizSHOPPERProbabilisticModel2019,
  title = {{{SHOPPER}}: {{A Probabilistic Model}} of {{Consumer Choice}} with {{Substitutes}} and {{Complements}}},
  shorttitle = {{{SHOPPER}}},
  author = {Ruiz, Francisco J. R. and Athey, Susan and Blei, David M.},
  year = {2019},
  month = jun,
  number = {arXiv:1711.03560},
  eprint = {1711.03560},
  primaryclass = {cs, econ, stat},
  publisher = {arXiv},
  urldate = {2023-10-14},
  abstract = {We develop SHOPPER, a sequential probabilistic model of shopping data. SHOPPER uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed SHOPPER to capture how items interact with other items. We develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. We are interested in answering counterfactual queries about changes in prices. We found that SHOPPER provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,notion,Statistics - Machine Learning},
  file = {/Users/hoener/Zotero/storage/T5ED88PX/Ruiz et al. - 2019 - SHOPPER A Probabilistic Model of Consumer Choice .pdf;/Users/hoener/Zotero/storage/L2AW5I45/1711.html}
}

@article{santiniMetaAnalysisLongShortTerm2016,
  title = {Meta-{{Analysis}} of the {{Long-}} and {{Short-Term Effects}} of {{Sales Promotions}} on {{Consumer Behavior}}},
  author = {Santini, Fernando De Oliveira and Vieira, Valter Afonso and Sampaio, Claudio Hoffmann and Perin, Marcelo Gattermann},
  year = {2016},
  month = may,
  journal = {Journal of Promotion Management},
  volume = {22},
  number = {3},
  pages = {425--442},
  issn = {1049-6491, 1540-7594},
  doi = {10.1080/10496491.2016.1154921},
  urldate = {2023-07-09},
  langid = {english},
  keywords = {notion}
}

@article{shaoSystematicalEvaluationNextBasket2022,
  title = {A {{Systematical Evaluation}} for {{Next-Basket Recommendation Algorithms}}},
  author = {Shao, Zhufeng and Wang, Shoujin and Zhang, Qian and Lu, Wenpeng and Li, Zhao and Peng, Xueping},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2209.02892},
  urldate = {2023-05-09},
  abstract = {Next basket recommender systems (NBRs) aim to recommend a user's next (shopping) basket of items via modeling the user's preferences towards items based on the user's purchase history, usually a sequence of historical baskets. Due to its wide applicability in the real-world E-commerce industry, the studies NBR have attracted increasing attention in recent years. NBRs have been widely studied and much progress has been achieved in this area with a variety of NBR approaches having been proposed. However, an important issue is that there is a lack of a systematic and unified evaluation over the various NBR approaches. Different studies often evaluate NBR approaches on different datasets, under different experimental settings, making it hard to fairly and effectively compare the performance of different NBR approaches. To bridge this gap, in this work, we conduct a systematical empirical study in NBR area. Specifically, we review the representative work in NBR and analyze their cons and pros. Then, we run the selected NBR algorithms on the same datasets, under the same experimental setting and evaluate their performances using the same measurements. This provides a unified framework to fairly compare different NBR approaches. We hope this study can provide a valuable reference for the future research in this vibrant area.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Information Retrieval (cs.IR),notion}
}

@article{shenBaselineNeedsMore2018,
  title = {Baseline {{Needs More Love}}: {{On Simple Word-Embedding-Based Models}} and {{Associated Pooling Mechanisms}}},
  shorttitle = {Baseline {{Needs More Love}}},
  author = {Shen, Dinghan and Wang, Guoyin and Wang, Wenlin and Min, Martin Renqiang and Su, Qinliang and Zhang, Yizhe and Li, Chunyuan and Henao, Ricardo and Carin, Lawrence},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1805.09843},
  urldate = {2023-07-09},
  abstract = {Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring a substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging. The source code and datasets can be obtained from https:// github.com/dinghanshen/SWEM.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),notion}
}

@article{simesterSurprisingBreadthHarbingers2019,
  title = {The {{Surprising Breadth}} of {{Harbingers}} of {{Failure}}},
  author = {Simester, Duncan I. and Tucker, Catherine E. and Yang, Clair},
  year = {2019},
  month = dec,
  journal = {Journal of Marketing Research},
  volume = {56},
  number = {6},
  pages = {1034--1049},
  issn = {0022-2437, 1547-7193},
  doi = {10.1177/0022243719867935},
  urldate = {2023-10-01},
  abstract = {Previous research has shown that there exist ``harbinger customers'' who systematically purchase new products that fail (and are discontinued by retailers). This article extends this result in two ways. First, the findings document the existence of ``harbinger zip codes.'' If households in these zip codes adopt a new product, this is a signal that the new product will fail. Second, a series of comparisons reveal that households in harbinger zip codes make other decisions that differ from other households. The first comparison identifies harbinger zip codes using purchases from one retailer and then evaluates purchases at a different retailer. Households in harbinger zip codes purchase products from the second retailer that other households are less likely to purchase. The analysis next compares donations to congressional election candidates; households in harbinger zip codes donate to different candidates than households in neighboring zip codes, and they donate to candidates who are less likely to win. House prices in harbinger zip codes also increase at slower rates than in neighboring zip codes. Investigation of households that change zip codes indicates that the harbinger zip code effect is more due to where customers choose to live, rather than households influencing their neighbors' tendencies.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Downloads/simester-et-al-2019-the-surprising-breadth-of-harbingers-of-failure.pdf;/Users/hoener/Zotero/storage/N9CYUKQN/Simester et al. - 2019 - The Surprising Breadth of Harbingers of Failure.pdf}
}

@article{sisodiaAutomaticallyDiscoveringVisual2022,
  title = {Automatically {{Discovering Visual Product Characteristics}}},
  author = {Sisodia, Ankit and Burnap, Alex and Kumar, Vineet},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4151019},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion}
}

@book{smolnik2017essays,
  title = {Essays on Market Response to Changes in Costs and Price Transparency},
  author = {Smolnik, Anna Olga},
  year = {2017},
  publisher = {Cuvillier Verlag},
  keywords = {notion}
}

@article{songHGATBRHyperedgebasedGraph2023,
  title = {{{HGAT-BR}}: {{Hyperedge-based}} Graph Attention Network for Basket Recommendation},
  shorttitle = {{{HGAT-BR}}},
  author = {Song, Tengshuo and Guo, Feng and Jiang, Haoran and Ma, Wenyun and Feng, Zhenbao and Guo, Lei},
  year = {2023},
  month = jan,
  journal = {Applied Intelligence},
  volume = {53},
  number = {2},
  pages = {1435--1451},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-022-03575-4},
  urldate = {2023-05-09},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Downloads/songHGATBRHyperedgebasedGraph2023.pdf}
}

@article{sun2003measuring,
  title = {Measuring the Impact of Promotions on Brand Switching When Consumers Are Forward Looking},
  author = {Sun, Baohong and Neslin, Scott A and Srinivasan, Kannan},
  year = {2003},
  journal = {Journal of Marketing Research},
  volume = {40},
  number = {4},
  pages = {389--405},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  keywords = {notion}
}

@incollection{sunHowFineTuneBERT2019,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  booktitle = {Chinese {{Computational Linguistics}}},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  editor = {Sun, Maosong and Huang, Xuanjing and Ji, Heng and Liu, Zhiyuan and Liu, Yang},
  year = {2019},
  volume = {11856},
  pages = {194--206},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32381-3_16},
  urldate = {2023-07-11},
  isbn = {978-3-030-32380-6 978-3-030-32381-3},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/5ZIIPAA6/Sun et al. - 2019 - How to Fine-Tune BERT for Text Classification.pdf}
}

@article{taleizadehBundlePricingInventory2020,
  title = {Bundle Pricing and Inventory Decisions on Complementary Products},
  author = {Taleizadeh, Ata Allah and Babaei, Masoumeh Sadat and Niaki, Seyed Taghi Akhavan and {Noori-daryan}, Mahsa},
  year = {2020},
  month = jun,
  journal = {Operational Research},
  volume = {20},
  number = {2},
  pages = {517--541},
  issn = {1109-2858, 1866-1505},
  doi = {10.1007/s12351-017-0335-4},
  urldate = {2023-06-26},
  langid = {english},
  keywords = {notion}
}

@article{thomasAugmentedHillClimbIncreases2022,
  title = {Augmented {{Hill-Climb}} Increases Reinforcement Learning Efficiency for Language-Based de Novo Molecule Generation},
  author = {Thomas, Morgan and O'Boyle, Noel M. and Bender, Andreas and {de Graaf}, Chris},
  year = {2022},
  month = oct,
  journal = {Journal of Cheminformatics},
  volume = {14},
  number = {1},
  pages = {68},
  issn = {1758-2946},
  doi = {10.1186/s13321-022-00646-z},
  urldate = {2024-04-07},
  abstract = {A plethora of AI-based techniques now exists to conduct de novo molecule generation that can devise molecules conditioned towards a particular endpoint in the context of drug design. One popular approach is using reinforcement learning to update a recurrent neural network or language-based de novo molecule generator. However, reinforcement learning can be inefficient, sometimes requiring up to 105 molecules to be sampled to optimize more complex objectives, which poses a limitation when using computationally expensive scoring functions like docking or computer-aided synthesis planning models. In this work, we propose a reinforcement learning strategy called Augmented Hill-Climb based on a simple, hypothesis-driven hybrid between REINVENT and Hill-Climb that improves sample-efficiency by addressing the limitations of both currently used strategies. We compare its ability to optimize several docking tasks with REINVENT and benchmark this strategy against other commonly used reinforcement learning strategies including REINFORCE, REINVENT (version 1 and 2), Hill-Climb and best agent reminder. We find that optimization ability is improved\,{\textasciitilde}\,1.5-fold and sample-efficiency is improved\,{\textasciitilde}\,45-fold compared to REINVENT while still delivering appealing chemistry as output. Diversity filters were used, and their parameters were tuned to overcome observed failure modes that take advantage of certain diversity filter configurations. We find that Augmented Hill-Climb outperforms the other reinforcement learning strategies used on six tasks, especially in the early stages of training or for more difficult objectives. Lastly, we show improved performance not only on recurrent neural networks but also on a reinforcement learning stabilized transformer architecture. Overall, we show that Augmented~Hill-Climb improves sample-efficiency for language-based de novo molecule generation conditioning via reinforcement learning, compared to the current state-of-the-art. This makes more computationally expensive scoring functions, such as docking, more accessible on a relevant timescale.},
  keywords = {AI,Artificial intelligence,De novo design,Deep learning,Generative models,Hill-Climb,Molecular docking,notion,Recurrent neural network,REINFORCE,Reinforcement learning,REINVENT,SBDD,Structure-based drug design},
  file = {/Users/hoener/Zotero/storage/WBAYY72Q/Thomas et al. - 2022 - Augmented Hill-Climb increases reinforcement learn.pdf;/Users/hoener/Zotero/storage/6WN3MGBB/s13321-022-00646-z.html}
}

@article{toubiaHowQuantifyingShape2021,
  title = {How Quantifying the Shape of Stories Predicts Their Success},
  author = {Toubia, Olivier and Berger, Jonah and Eliashberg, Jehoshua},
  year = {2021},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {26},
  pages = {e2011695118},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2011695118},
  urldate = {2023-09-26},
  abstract = {Significance             Why are some narratives (e.g., movies) or other texts (e.g., academic papers) more successful than others? Narratives are often described as moving quickly, covering lots of ground, or going in circles, but little work has quantified such movements or tested whether they might explain success. We use natural language processing and machine learning to analyze the content of almost 50,000 texts, constructing a simple set of measures (i.e., speed, volume, and circuitousness) that quantify the semantic progression of discourse. While movies and TV shows that move faster are liked more, TV shows that cover more ground are liked less. Academic papers that move faster are cited less, and papers that cover more ground or are more circuitous are cited more.           ,              Narratives, and other forms of discourse, are powerful vehicles for informing, entertaining, and making sense of the world. But while everyday language often describes discourse as moving quickly or slowly, covering a lot of ground, or going in circles, little work has actually quantified such movements or examined whether they are beneficial. To fill this gap, we use several state-of-the-art natural language-processing and machine-learning techniques to represent texts as sequences of points in a latent, high-dimensional semantic space. We construct a simple set of measures to quantify features of this semantic path, apply them to thousands of texts from a variety of domains (i.e., movies, TV shows, and academic papers), and examine whether and how they are linked to success (e.g., the number of citations a paper receives). Our results highlight some important cross-domain differences and provide a general framework that can be applied to study many types of discourse. The findings shed light on why things become popular and how natural language processing can provide insight into cultural success.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/54IKVBCX/Toubia et al. - 2021 - How quantifying the shape of stories predicts thei.pdf}
}

@article{trofimovInferringComplementaryProducts2018,
  title = {Inferring {{Complementary Products}} from {{Baskets}} and {{Browsing Sessions}}},
  author = {Trofimov, Ilya},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1809.09621},
  urldate = {2023-06-26},
  abstract = {Complementary products recommendation is an important problem in e-commerce. Such recommendations increase the average order price and the number of products in baskets. Complementary products are typically inferred from basket data. In this study, we propose the BB2vec model. The BB2vec model learns vector representations of products by analyzing jointly two types of data - Baskets and Browsing sessions (visiting web pages of products). These vector representations are used for making complementary products recommendation. The proposed model alleviates the cold start problem by delivering better recommendations for products having few or no purchases. We show that the BB2vec model has better performance than other models which use only basket data.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Information Retrieval (cs.IR),Machine Learning (cs.LG),Machine Learning (stat.ML),notion}
}

@article{tversky1981framing,
  title = {The Framing of Decisions and the Psychology of Choice},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1981},
  journal = {Science (New York, N.Y.)},
  volume = {211},
  number = {4481},
  pages = {453--458},
  publisher = {American Association for the Advancement of Science},
  keywords = {notion}
}

@unknown{unknown,
  title = {Improve the Interpretability of Attention: {{A}} Fast, Accurate, and Interpretable High-Resolution Attention Model},
  author = {Gomez, Tristan and Ling, Suiyi and Fr{\'e}our, Thomas and Mouch{\`e}re, Harold},
  year = {2021},
  month = jun,
  keywords = {notion}
}

@article{uppadaImageTextbasedMultimodal2023,
  title = {An Image and Text-Based Multimodal Model for Detecting Fake News in {{OSN}}'s},
  author = {Uppada, Santosh Kumar and Patel, Parth and B., Sivaselvan},
  year = {2023},
  month = oct,
  journal = {Journal of Intelligent Information Systems},
  volume = {61},
  number = {2},
  pages = {367--393},
  issn = {1573-7675},
  doi = {10.1007/s10844-022-00764-y},
  urldate = {2024-02-25},
  abstract = {Digital Mass Media has become the new paradigm of communication that revolves around online social networks. The increase in the utilization of online social networks (OSNs) as the primary source of information and the increase of online social platforms providing such news has increased the scope of spreading fake news. People spread fake news in multimedia formats like images, audio, and video. Visual-based news is prone to have a psychological impact on the users and is often misleading. Therefore, Multimodal frameworks for detecting fake posts have gained demand in recent times. This paper proposes a framework that flags fake posts with Visual data embedded with text. The proposed framework works on data derived from the Fakeddit dataset, with over 1 million samples containing text, image, metadata, and comments data gathered from a wide range of sources, and tries to exploit the unique features of fake and legitimate images. The proposed framework has different architectures to learn visual and linguistic models from the post individually. Image polarity datasets, derived from Flickr, are also considered for analysis, and the features extracted from these visual and text-based data helped in flagging news. The proposed fusion model has achieved an overall accuracy of 91.94\%, Precision of 93.43\%, Recall of 93.07\%, and F1-score of 93\%. The experimental results show that the proposed Multimodality model with Image and Text achieves better results than other state-of-art models working on a similar dataset.},
  langid = {english},
  keywords = {Bert + Dense,Click-baits,Error level analysis,Fake news detection,Fakeddit,Fusion models,notion,Visual sentiment analysis,Xception},
  file = {/Users/hoener/Zotero/storage/JXSGHSYU/Uppada et al. - 2023 - An image and text-based multimodal model for detec.pdf}
}

@article{valleMarketBasketAnalysis2019,
  title = {Market Basket Analysis by Solving the Inverse {{Ising}} Problem: {{Discovering}} Pairwise Interaction Strengths among Products},
  shorttitle = {Market Basket Analysis by Solving the Inverse {{Ising}} Problem},
  author = {Valle, Mauricio A. and Ruz, Gonzalo A. and Rica, Sergio},
  year = {2019},
  month = jun,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {524},
  pages = {36--44},
  issn = {03784371},
  doi = {10.1016/j.physa.2019.03.001},
  urldate = {2023-06-26},
  langid = {english},
  keywords = {notion}
}

@article{vaneverdingenGearManufacturersContestants2019,
  title = {Gear {{Manufacturers}} as {{Contestants}} in {{Sports Competitions}}: {{Breeding}} and {{Branding Returns}}},
  shorttitle = {Gear {{Manufacturers}} as {{Contestants}} in {{Sports Competitions}}},
  author = {Van Everdingen, Yvonne and Hariharan, Vijay Ganesh and Stremersch, Stefan},
  year = {2019},
  month = may,
  journal = {Journal of Marketing},
  volume = {83},
  number = {3},
  pages = {126--144},
  issn = {0022-2429, 1547-7185},
  doi = {10.1177/0022242919831996},
  urldate = {2023-10-15},
  abstract = {Several manufacturers make substantial investments to compete in sports contests, using the gear they develop and market. However, no systematic analysis of the breeding (i.e., innovation) and branding (i.e., marketing) returns from such investments exists. In this study, the authors conceptualize and empirically estimate the breeding and branding returns that such manufacturers obtain. The authors gather data for 30 car brands of 16 manufacturers over the period 2000--2015 regarding their participation, spending, and performance in Formula One championships, annual patent citations, and research-and-development (R\&D) budgets as well as monthly vehicle registrations, advertising expenditures, and Formula One TV viewership. The authors find that only gear manufacturers with relatively high levels of R\&D spending obtain a positive and significant breeding return from competing in sports contests. While most brands obtain positive branding returns, the lower the level of advertising spending for the brand, the greater the branding returns they obtain from competing in these contests. Thus, research-intense (compared with advertising-intense) gear manufacturers have more to gain from competing in sports contests. These findings can help guide manufacturers in budget allocation decisions on sports competitions, R\&D, and advertising.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/MMBKRFI2/Van Everdingen et al. - 2019 - Gear Manufacturers as Contestants in Sports Compet.pdf}
}

@article{vanmaasakkersNextbasketPredictionHighdimensional2023,
  title = {Next-Basket Prediction in a High-Dimensional Setting Using Gated Recurrent Units},
  author = {{van Maasakkers}, Luuk and Fok, Dennis and Donkers, Bas},
  year = {2023},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {212},
  pages = {118795},
  issn = {09574174},
  doi = {10.1016/j.eswa.2022.118795},
  urldate = {2023-04-26},
  abstract = {Accurately predicting the next shopping basket of a customer is important for retailers, as it offers an opportunity to serve customers with personalized product recommendations or shopping lists. The goal of next-basket prediction is to predict a coherent set of products that the customer will buy next, rather than just a single product. However, if the assortment of the retailer contains thousands of products, the number of possible baskets becomes extremely large and most standard choice models can no longer be applied. Therefore, we propose the use of a gated recurrent unit (GRU) network for next-basket prediction in this study, which is easily scalable to large assortments. Our proposed model is able to capture dynamic customer taste, recurrency in purchase behavior and frequent product co-occurrences in shopping baskets. Moreover, it allows for the inclusion of additional covariates. Using two real-life datasets, we demonstrate that our model is able to outperform both naive benchmarks and a state-of-the-art next-basket prediction model on several performance measures. We also illustrate that the model learns meaningful patterns about the retailer's assortment structure.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/32D789VX/van Maasakkers et al. - 2023 - Next-basket prediction in a high-dimensional setti.pdf}
}

@article{vaswani2017attention,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {notion}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-26},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {/Users/hoener/Zotero/storage/2FFGXKS5/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@misc{vosoughiLargescaleKernelizedGRANGER2020,
  title = {Large-Scale Kernelized {{GRANGER}} Causality to Infer Topology of Directed Graphs with Applications to Brain Networks},
  author = {Vosoughi, M. Ali and Wismuller, Axel},
  year = {2020},
  month = nov,
  number = {arXiv:2011.08261},
  eprint = {2011.08261},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-12},
  abstract = {Graph topology inference of network processes with co-evolving and interacting time-series is crucial for network studies. Vector autoregressive models (VAR) are popular approaches for topology inference of directed graphs; however, in large networks with short time-series, topology estimation becomes ill-posed. The present paper proposes a novel nonlinearity-preserving topology inference method for directed networks with co-evolving nodal processes that solves the ill-posedness problem. The proposed method, large-scale kernelized Granger causality (lsKGC), uses kernel functions to transform data into a low-dimensional feature space and solves the autoregressive problem in the feature space, then finds the pre-images in the input space to infer the topology. Extensive simulations on synthetic datasets with nonlinear and linear dependencies and known ground-truth demonstrate significant improvement in the Area Under the receiver operating characteristic Curve ( AUC ) of the receiver operating characteristic for network recovery compared to existing methods. Furthermore, tests on real datasets from a functional magnetic resonance imaging (fMRI) study demonstrate 96.3 percent accuracy in diagnosis tasks of schizophrenia patients, which is the highest in the literature with only brain time-series information.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/hoener/Zotero/storage/VT59GKE4/Vosoughi and Wismuller - 2020 - Large-scale kernelized GRANGER causality to infer .pdf;/Users/hoener/Zotero/storage/LFILBY3G/2011.html}
}

@inproceedings{wang2015learning,
  title = {Learning Hierarchical Representation Model for Nextbasket Recommendation},
  booktitle = {Proceedings of the 38th International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Wang, Pengfei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Wan, Shengxian and Cheng, Xueqi},
  year = {2015},
  pages = {403--412},
  keywords = {notion}
}

@phdthesis{wang2021low,
  title = {Low Engagement and Failed Choices: Exploring the Mechanism for Harbingers of Failure},
  author = {Wang, Yifei and others},
  year = {2021},
  school = {Massachusetts Institute of Technology},
  keywords = {notion}
}

@article{wangEndogenousConsumptionMetered2023,
  title = {Endogenous {{Consumption}} and {{Metered Paywalls}}},
  author = {Wang, Chutian and Zhou, Bo and Joshi, Yogesh V.},
  year = {2023},
  month = may,
  journal = {Marketing Science},
  pages = {mksc.2023.1444},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.2023.1444},
  urldate = {2023-10-16},
  abstract = {This paper studies the optimal design of a paywall when consumers endogenously determine their amount of content consumption.           ,              Digital content platforms increasingly implement paywalls to generate subscription revenue, but putting content behind paywalls can reduce consumption and advertising revenue. We analyze optimal paywall design when consumers are heterogenous in their valuation for and satiation with content and endogenously determine their consumption. We find that, under moderate ad rates, a metered paywall offering a limited amount of free content is optimal when consumers display sufficient heterogeneity in satiation. When ad rates increase, one might expect platforms to offer more free content to generate higher ad revenue. Instead, we find that sometimes offering less free content and lowering the subscription price is optimal to attract new subscribers. Further, depending on ad rates, the optimal amount of free content may increase or decrease in consumers' valuation for content. As valuation increases, consumers' willingness to pay for subscription and desire for content consumption increases. The former effect dominates under low ad rates and incentivizes the platform to provide less free content, but the opposite is true under high ad rates. Counterintuitively, we also find that total content consumption can increase with the proportion of users who become satiated relatively faster because the platform may strategically raise its meter limit to increase ad revenues from nonsubscribers. We also explore the impact of a more appealing outside option for consumers, competition between media platforms, and correlation between valuation and satiation on the optimality of paywalls.             History: Yuxin Chen served as the senior editor.             Funding: This work was supported by the INFORMS Society for Marketing Science (ISMS) through its ISMS Doctoral Dissertation Early Stage Research Grants.             Supplemental Material: The online appendix is available at https://doi.org/10.1287/mksc.2023.1444 .},
  langid = {english},
  keywords = {notion}
}

@inproceedings{wangLearningHierarchicalRepresentation2015,
  title = {Learning {{Hierarchical Representation Model}} for {{NextBasket Recommendation}}},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Wang, Pengfei and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Wan, Shengxian and Cheng, Xueqi},
  year = {2015},
  month = aug,
  pages = {403--412},
  publisher = {ACM},
  address = {Santiago Chile},
  doi = {10.1145/2766462.2767694},
  urldate = {2023-05-09},
  isbn = {978-1-4503-3621-5},
  langid = {english},
  keywords = {notion}
}

@article{wangModelingUserDemand2022,
  title = {Modeling {{User Demand Evolution}} for {{Next-basket Prediction}}},
  author = {Wang, Shoujin and Wang, Yan and Hu, Liang and Zhang, Xiuzhen and Zhang, Qi and Sheng, Quan Z. and Orgun, Mehmet A. and Cao, Longbing and Lian, Defu},
  year = {2022},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--14},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3231018},
  urldate = {2023-05-09},
  keywords = {notion}
}

@article{wangTSDAEUsingTransformerbased2021,
  title = {{{TSDAE}}: {{Using Transformer-based Sequential Denoising Auto-Encoder}} for {{Unsupervised Sentence Embedding Learning}}},
  shorttitle = {{{TSDAE}}},
  author = {Wang, Kexin and Reimers, Nils and Gurevych, Iryna},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2104.06979},
  urldate = {2023-07-09},
  abstract = {Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1\% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,notion}
}

@article{weberConstrainedNeuralNetwork2021,
  title = {Constrained Neural Network Training and Its Application to Hyperelastic Material Modeling},
  author = {Weber, Patrick and Geiger, Jeremy and Wagner, Werner},
  year = {2021},
  month = nov,
  journal = {Computational Mechanics},
  volume = {68},
  number = {5},
  pages = {1179--1204},
  issn = {0178-7675, 1432-0924},
  doi = {10.1007/s00466-021-02064-8},
  urldate = {2023-10-17},
  abstract = {Abstract             Neural networks (NN) have been studied and used widely in the field of computational mechanics, especially to approximate material behavior. One of their disadvantages is the large amount of data needed for the training process. In this paper, a new approach to enhance NN training with physical knowledge using constraint optimization techniques is presented. Specific constraints for hyperelastic materials are introduced, which include energy conservation, normalization and material symmetries. We show, that the introduced enhancements lead to better learning behavior with respect to well known issues like a small number of training samples or noisy data. The NN is used as a material law within a finite element analysis and its convergence behavior is discussed with regard to the newly introduced training enhancements. The feasibility of NNs trained with physical constraints is shown for data based on real world experiments. We show, that the enhanced training outperforms state-of-the-art techniques with respect to stability and convergence behavior within FE simulations.},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/G2YN9EZB/Weber et al. - 2021 - Constrained neural network training and its applic.pdf}
}

@inproceedings{weteringsInterpretingAttentionModels2020,
  title = {Interpreting {{Attention Models}}: {{LSTM}} vs. {{CNN}} : {{A}} Case Study on Customer Activation},
  shorttitle = {Interpreting {{Attention Models}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Weterings, Koen and Kimelman, Shir-Lee and Bromuri, Stefano and Van Eekelen, Marko},
  year = {2020},
  month = dec,
  pages = {336--343},
  publisher = {IEEE},
  address = {Miami, FL, USA},
  doi = {10.1109/ICMLA51294.2020.00061},
  urldate = {2023-06-26},
  isbn = {978-1-72818-470-8},
  keywords = {notion}
}

@article{whitePromptPatternCatalog2023,
  title = {A {{Prompt Pattern Catalog}} to {{Enhance Prompt Engineering}} with {{ChatGPT}}},
  author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and {Spencer-Smith}, Jesse and Schmidt, Douglas C.},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.11382},
  urldate = {2023-07-10},
  abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,notion,Software Engineering (cs.SE)}
}

@article{williamsAlgorithmicGhostResearch2023,
  title = {Algorithmic {{Ghost}} in the {{Research Shell}}: {{Large Language Models}} and {{Academic Knowledge Creation}} in {{Management Research}}},
  shorttitle = {Algorithmic {{Ghost}} in the {{Research Shell}}},
  author = {Williams, Nigel and Ivanov, Stanislav and Buhalis, Dimitrios},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.07304},
  urldate = {2023-06-26},
  abstract = {The paper looks at the role of large language models in academic knowledge creation based on a scoping review (2018 to January 2023) of how researchers have previously used the language model GPT to assist in the performance of academic knowledge creation tasks beyond data analysis. These tasks include writing, editing, reviewing, dataset creation and curation, which have been difficult to perform using earlier ML tools. Based on a synthesis of these papers, this study identifies pathways for a future academic research landscape that incorporates wider usage of large language models based on the current modes of adoption in published articles as a Co-Writer, Research Assistant and Respondent.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,K.3; K.4; K.6,notion}
}

@article{xuBundlingStrategiesComplementary2018,
  title = {Bundling Strategies for Complementary Products in a Horizontal Supply Chain},
  author = {Xu, Qingyun and Xu, Bing and Wang, Ping and He, Yi},
  year = {2018},
  month = jun,
  journal = {Kybernetes},
  volume = {47},
  number = {6},
  pages = {1158--1177},
  issn = {0368-492X},
  doi = {10.1108/K-02-2017-0082},
  urldate = {2023-06-26},
  abstract = {Purpose               This paper aims to address the following problems: What are the firms' optimal pricing and quality policies under three scenarios (no bundling, pure bundling and mixed bundling)? In what condition will one bundling strategy dominate the others? How does the degree of complementarity affect the firms' decision?                                         Design/methodology/approach               Using the game theory, this study first establishes three models of bundling strategies: no bundling, pure bundling and mixed bundling and then obtains the optimal prices and quality decisions. This study uses numerical analysis to explore the relationships between the prices (demands and profits) and some key parameters and to obtain some valuable management complications.                                         Findings               Some interesting and valuable management implications are established: regardless of the degree of complementarity, adopting a pure bundling or mixed bundling strategy is better than separately selling an individual product; a high degree of complementarity leads to reduced profit in the no bundling and mixed bundling scenarios, whereas the condition in the pure bundling strategy is the opposite; and when the degree of complementarity is adequately large, choosing pure bundling strategy is more profitable.                                         Research limitations/implications               On the one hand, this study does not calculate the profit sharing ratio, and hence, the equilibrium profit sharing ratio can be explored in future work. On the other hand, marketing efforts (e.g. advertising and promotion) can be included in the study.                                         Practical implications               This study derives the necessary conditions for the most effective bundling strategy that maximizes firm's profits, and these conclusions can provide a decision reference to the bundling decisions of firms.                                         Originality/value               First, the optimal bundling strategies in a horizontal supply chain consisting of two firms is considered. Under the pure and mixed bundling strategies, the two firms sell the bundled product by building a cooperative program. Second, both the pricing policies and quality decisions of supply chain members under the different bundling strategies are studied.},
  langid = {english},
  keywords = {notion}
}

@article{yangEstimatingValueBrand2009,
  title = {Estimating the {{Value}} of {{Brand Alliances}} in {{Professional Team Sports}}},
  author = {Yang, Yupin and Shi, Mengze and Goldfarb, Avi},
  year = {2009},
  month = nov,
  journal = {Marketing Science},
  volume = {28},
  number = {6},
  pages = {1095--1111},
  issn = {0732-2399, 1526-548X},
  doi = {10.1287/mksc.1090.0513},
  urldate = {2023-10-14},
  abstract = {Brands often form alliances to enhance their brand equities. In this paper, we examine the alliances between professional athletes (athlete brands) and sports teams (team brands) in the National Basketball Association (NBA). Athletes and teams match to maximize the total added value created by the brand alliance. To understand this total value, we estimate a structural two-sided matching model using a maximum score method. Using data on the free-agency contracts signed in the NBA during the four-year period from 1994 to 1997, we find that both older players and players with higher performance are more likely to match with teams with more wins. However, controlling for performance, we find that brand alliances between high brand equity players (defined as receiving enough votes to be an all-star starter) and medium brand equity teams (defined by stadium and broadcast revenues) generate the highest value. This suggests that top brands are not necessarily best off matching with other top brands. We also provide suggestive evidence that the maximum salary policy implemented in 1998 influenced matches based on brand equity spillovers more than matches based on performance complementarities.},
  langid = {english},
  keywords = {notion}
}

@misc{yiStock2VecEmbeddingImprove2022,
  title = {{{Stock2Vec}}: {{An Embedding}} to {{Improve Predictive Models}} for {{Companies}}},
  shorttitle = {{{Stock2Vec}}},
  author = {Yi, Ziruo and Xiao, Ting and Ijeoma, Kaz-Onyeakazi and Cheran, Ratnam and Baweja, Yuvraj and Nelson, Phillip},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11290},
  eprint = {2201.11290},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-14},
  abstract = {Building predictive models for companies often relies on inference using historical data of companies in the same industry sector. However, companies are similar across a variety of dimensions that should be leveraged in relevant prediction problems. This is particularly true for large, complex organizations which may not be well defined by a single industry and have no clear peers. To enable prediction using company information across a variety of dimensions, we create an embedding of company stocks, Stock2Vec, which can be easily added to any prediction model that applies to companies with associated stock prices. We describe the process of creating this rich vector representation from stock price fluctuations, and characterize what the dimensions represent. We then conduct comprehensive experiments to evaluate this embedding in applied machine learning problems in various business contexts. Our experiment results demonstrate that the four features in the Stock2Vec embedding can readily augment existing cross-company models and enhance cross-company predictions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion},
  file = {/Users/hoener/Zotero/storage/AVWNARJW/Yi et al. - 2022 - Stock2Vec An Embedding to Improve Predictive Mode.pdf;/Users/hoener/Zotero/storage/7SU996ML/2201.html}
}

@inproceedings{yuDynamicRecurrentModel2016,
  title = {A {{Dynamic Recurrent Model}} for {{Next Basket Recommendation}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Yu, Feng and Liu, Qiang and Wu, Shu and Wang, Liang and Tan, Tieniu},
  year = {2016},
  month = jul,
  pages = {729--732},
  publisher = {ACM},
  address = {Pisa Italy},
  doi = {10.1145/2911451.2914683},
  urldate = {2023-05-09},
  isbn = {978-1-4503-4069-4},
  langid = {english},
  keywords = {notion}
}

@misc{yuTemporalDataMeets2023,
  title = {Temporal {{Data Meets LLM}} -- {{Explainable Financial Time Series Forecasting}}},
  author = {Yu, Xinli and Chen, Zheng and Ling, Yuan and Dong, Shujing and Liu, Zongyi and Lu, Yanbin},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11025},
  eprint = {2306.11025},
  primaryclass = {cs, q-fin},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,F.2.2,I.2.1,I.2.7,notion,Quantitative Finance - Statistical Finance},
  file = {/Users/hoener/Zotero/storage/IN8PV2CN/Yu et al. - 2023 - Temporal Data Meets LLM -- Explainable Financial T.pdf;/Users/hoener/Zotero/storage/K74RVZRW/2306.html}
}

@article{zhang2009effectiveness,
  title = {The Effectiveness of Customized Promotions in Online and Offline Stores},
  author = {Zhang, Jie and Wedel, Michel},
  year = {2009},
  journal = {Journal of marketing research},
  volume = {46},
  number = {2},
  pages = {190--206},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  keywords = {notion}
}

@article{zhangATLSTMAttentionbasedLSTM2019,
  title = {{{AT-LSTM}}: {{An Attention-based LSTM Model}} for {{Financial Time Series Prediction}}},
  shorttitle = {{{AT-LSTM}}},
  author = {Zhang, Xuan and Liang, Xun and Zhiyuli, Aakas and Zhang, Shusen and Xu, Rui and Wu, Bo},
  year = {2019},
  month = jul,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {569},
  number = {5},
  pages = {052037},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/569/5/052037},
  urldate = {2023-10-09},
  abstract = {Abstract             This paper proposes an attention-based LSTM (AT-LSTM) model for financial time series prediction. We divide the prediction process into two stages. For the first stage, we apply an attention model to assign different weights to the input features of the financial time series at each time step. In the second stage, the attention feature is utilized to effectively select the relevant feature sequences as input to the LSTM neural network for the prediction in the next time frame. Our proposed framework not only solves the long-term dependence problem of time series prediction effectively, but also improves the interpretability of the time series prediction methods based on the neural network. In the end of this paper, we conducted experiments on financial time series prediction task with three real-world data sets. The experimental results show that our framework for time series pre-diction is state-of-the-art against the baselines.},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/MYZ2JYUT/Zhang et al. - 2019 - AT-LSTM An Attention-based LSTM Model for Financi.pdf}
}

@article{zhangATLSTMAttentionbasedLSTM2019a,
  title = {{{AT-LSTM}}: {{An Attention-based LSTM Model}} for {{Financial Time Series Prediction}}},
  shorttitle = {{{AT-LSTM}}},
  author = {Zhang, Xuan and Liang, Xun and Zhiyuli, Aakas and Zhang, Shusen and Xu, Rui and Wu, Bo},
  year = {2019},
  month = jul,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {569},
  number = {5},
  pages = {052037},
  issn = {1757-8981, 1757-899X},
  doi = {10.1088/1757-899X/569/5/052037},
  urldate = {2023-10-15},
  abstract = {Abstract             This paper proposes an attention-based LSTM (AT-LSTM) model for financial time series prediction. We divide the prediction process into two stages. For the first stage, we apply an attention model to assign different weights to the input features of the financial time series at each time step. In the second stage, the attention feature is utilized to effectively select the relevant feature sequences as input to the LSTM neural network for the prediction in the next time frame. Our proposed framework not only solves the long-term dependence problem of time series prediction effectively, but also improves the interpretability of the time series prediction methods based on the neural network. In the end of this paper, we conducted experiments on financial time series prediction task with three real-world data sets. The experimental results show that our framework for time series pre-diction is state-of-the-art against the baselines.},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/YZ4Z2DKC/Zhang et al. - 2019 - AT-LSTM An Attention-based LSTM Model for Financi.pdf}
}

@article{zhaoConnectingSocialMedia2016,
  title = {Connecting {{Social Media}} to {{E-Commerce}}: {{Cold-Start Product Recommendation Using Microblogging Information}}},
  shorttitle = {Connecting {{Social Media}} to {{E-Commerce}}},
  author = {Zhao, Wayne Xin and Li, Sui and He, Yulan and Chang, Edward Y. and Wen, Ji-Rong and Li, Xiaoming},
  year = {2016},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {28},
  number = {5},
  pages = {1147--1159},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2015.2508816},
  urldate = {2023-07-09},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/S8QC2U2P/Zhao et al. - 2016 - Connecting Social Media to E-Commerce Cold-Start .pdf}
}
