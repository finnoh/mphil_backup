
Our research is relevant to the Creative Industry, as our algorithm can be used to personalize text media to specific consumer's of these media. Also, the novel approach that we propose of estimating a generative summary embedding, might find applications in this area to represent the information captured in a piece of writing.

#	Description

This project proposes an new method to estimate summary embeddings for Large Language Models (LLMs), that enable them to generate specific output sequences. We apply this novel method to tailor advertising claims, harnessing these "summary embeddings." Essentially, our technique reverse-engineers the generation of advertising claims into unique vector representations, which we correlate with market preferences for these claims. Additionally, we delve into the interaction between brands and these summary embeddings within the generation space. Our project aims to refine advertising strategies and identify market gaps, akin to perceptual maps. The potential marketing applications for these summary embeddings are extensive. Beyond personalizing advertising claims, they can potentially enhance recommendation engines and help with website morphing. Unlike existing personalization methods, our approach doesn't rely on a predefined set of offerings but can generate new offerings that weren't part of the training data.

# Scientific project description

The crux of our proposed method revolves around understanding how Language Models (LLMs) generate textâ€”namely, by predicting the next token in a sequence of tokens. Our approach involves estimating summary embeddings, which serve as the input embeddings to the LLM. These embeddings are designed to maximize the likelihood of the LLM generating a specific output sequence. Through an autoencoder, we can model these input embeddings, capturing them in a shared, compact "generation space." Within this space, we can explore various points to generate new claims, evaluate their quality, and gauge the suitability of a summary embedding for a particular claim.

Following validation on a small dataset, our next step involves scaling up to a larger dataset sourced from a market research firm. These data consist of aggregated ratings provided by respondents regarding advertising claims, with no individual personal information included. Additionally, we possess metadata pertaining to these advertising claims, such as the associated brand. Our intention is to integrate this metadata, including brand information, into our autoencoder model. However, such computational tasks demand more power than a personal laptop can offer, making GPU resources necessary. The support of a NWO small compute grant, would enable us to pursue this project further. 
 
# Technical project requirements

A typical application run on a CPU takes 36 hours and uses less than 16GB of memory, with a small LLM (GPT-2). Since, we will be computing on a GPU, we expect it to take around 12h on a GPU, as we observed a speed-up by a factor of 3 on Google Colab's T4 GPU. The input data is less than 1 GB in size. The application uses around 10 software packages in Python to execute the runs, among them the pytorch and transformers packages. The application runs are independent of each other. Since, we are developing and exploring a novel algorithm, we cannot put a fixed amount of runs as an upper bound. However, for our current application, we expect to need about 150 runs. We plan to employ a larger LLM (e.g. LLAMA 3) in our research, which is more computationally demanding than GPT-2. LLAMA 3 8B requires around 16GB of disk space and 20GB of VRAM, this is about 4 times more than GPT-2. For this reason, and to account for further developments of our algorithm, we add another 50% on top of this amount (12h x 150runs = 1,800 GPU hours) in our budget request (i.e. 2,700 GPU hours). 

345,600 SBUs, 2,700 GPU hours