---
author: Finn-Ole HÃ¶ner (657110)
title: "MPhil"
date: last-modified
date-format: long
code-color-bg: lightgray
header-includes:
  \usepackage{fancyhdr}
  \usepackage{fancyvrb}
  \usepackage{float}
  \usepackage{booktabs}
  \usepackage{lscape}
  \usepackage{amsmath}
  \usepackage{graphicx}
  \usepackage{algorithm}
  \usepackage{algpseudocode}
  \usepackage{kpfonts}
  \DeclareMathOperator*{\argmin}{arg\,min}
  \DeclareMathOperator*{\argmax}{arg\,max}
pdf-engine: pdflatex
format:
    pdf:
        documentclass: article
        toc: false
        number-sections: false
        number-depth: 2
        colorlinks: true
        fig-pos: 'H'
        fig-align: center
        geometry:
            - top = 20mm
            - bottom = 20mm
            - left = 20mm
            - right = 20mm
        papersize: dina4
bibliography: references.bib
echo: false
eval: true
messages: false
error: false
warning: false
cache: false
---

# Introduction

- Media products increase relevance of content
  - Social media, short videos TikTok, Images, Netflix, Audio / Podcasts

- What is missing from preference learning? How is this different?
  - Media products / things are different from products as attribute bundles
    - We can directly represent the product itself?
  - Brands and their role in advertising

- Market research is expensive. How can we leverage these insights better to improve its benefits?

- EU AI Act

- GDPR
  - Challenges to cookies in website morphing

- Generative AI for personalization
  - Previously: Set pieces, e.g. recommendation systems
  - True personalization: Generate solution for each individual consumer

- Consumer preferences in latent generation space
  - We can view products as attribute bundles and get utilities for these attributes <!-- TODO: What about interactions of attributes? -->
  - How do we do this for products that cannot be "discretized"?

- Novel training framework
  - Circumvent issues with the LLMs decoder
  - Input embedding represents the information contained in the sequence, as it is the starting point which makes the LLM generate the target sequence


<!-- TODO: Consider renaming this headline -->

# Theory

- Generative AI for text
    - Attention
    - Transformer
    - GPT

- Autoencoder

- Maximize the likelihood to generate the data

The Autoencoder $\operatorname{AE}_{\mathcal{B}} \left(\cdot \right): \lbrace 0, 1 \rbrace^{C} \to \mathbb{R}^E$, with parameters $\mathcal{B}$

The log-likelihood function $\operatorname{\mathcal{L}} \left ( \cdot \right): \mathbb{R}^E \to \left (- \infty, 0 \right]$

The length $E$ summary-embedding $\boldsymbol{e}$.

The length $T$ vector of target tokens $\boldsymbol{t}$, where the element $t_i$ is a unique integer code representing a token, e.g. in GPT-2, $50267$ is the End-of-Text token `<|eos|>`.

<!-- TODO: What does it really return? -->
<!-- TODO: Good idea or not? -->
To express that we generate tokens with the LLM, $\mathcal{M}$, based on length $E$ input-embedding $\boldsymbol{e}$, we use the notation $\mathcal{M}(\boldsymbol{e})$. To express the number of tokens, $k$, that we generate, we use the subscript $\mathcal{M}_k(\boldsymbol{e})$. This function call returns an output vector of length $V$, where $V$ denotes the vocabulary size. We denote the output vector as $\boldsymbol{o}$. Each element $o_v$ of this output vector represents the probability of generating token $v$ as the next token. We predict the next token, by selecting the largest element of $\boldsymbol{o}$. When $k = 1$, we use the previous tokens to predict the next token. When $k > 1$, we append the predicted tokens autoregressively to the input embedding, such that we can predict more than one token. While there are many ways to design this procedure and select tokens along the way, we perform a standard greedy procedure, where we select the token with the highest probability at each step. We denote the probability that $\mathcal{M}$ generates the target sequence, given the input embedding, as $p(t_1, \ldots, t_L | \boldsymbol{e})$. For a given Large Language Model, we define $\boldsymbol{e^{*}}$,

$$
\boldsymbol{e^{*}} = \argmax_{\boldsymbol{e}} \sum_{i=1}^{L} \log p(t_1, \ldots, t_L | \boldsymbol{e}) \text{, }
$$

as the input-embedding that maximizes the likelihood of generating a given target sequence, $\lbrace t_i \rbrace_{i=1}^{L}$, where $L$ denotes the length of the target sequence and $t_i$ represents token $i$. We estimate these summary embeddings $\boldsymbol{e^{*}}$, by maximizing the conditional likelihood that this embedding generates the target sequence for a given LLM. We use a gradient-based optimization algorithm to maximize this likelihood.

- Adding covariates to the AE


- LLM judge to rate claims

<!-- TODO: Consider renaming this headline -->
# Implementation

- Autoencoder

- Configuration of LLM

- Optimization

- Connection to other measures, e.g. syntax or rating of consumers

- Different generation strategies, use beam search to get closer to max LL

# Data

- Descriptives of the data

- Prepocessing of the data

# Results

- Optimization
  - Computation time, scaling
- Encoding space
  - example tangible, intangible
  - generations from encoding space
  - Wasteland, geometrical shape and where it comes from
- Measures of fit
  - areas in the encoding space (count out the pixels and compare to curvature of LL?)
  - analysis of the trees
  - Gap between most likely and second most likely token
- Application to MR data

- Comparison to BERT embeddings

# Managerial Implications

- Generative AI for personalization
- Brands
- Wording

# Discussion

- Problems
  - Computation time

- Comparison to BERT embeddings

- Expansion of this research
  - MAB around the fitted AE, to incorporate consumer feedback in online learning: Which areas of the space are valuable for which consumers?
  - Better LLM
  - Image, video, website, audio data. Perhaps website a good application. Perhaps audio a rather simple way to make this multi-modal (content and sound)