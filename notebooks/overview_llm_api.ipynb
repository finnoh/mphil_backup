{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLM API Overview\"\n",
    "format: pdf\n",
    "code-overflow: wrap\n",
    "messages: false\n",
    "outputs: false\n",
    "warnings: false\n",
    "errors: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a test string and specify a model to use for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test string and specify a model to use for the test.\n",
    "s_string = \"Finn writes code\"\n",
    "s_model = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoener/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   284 |  to      | -1.336 | 26.28%\n",
      "|   787 |  make    | -3.183 | 4.15%\n",
      "|   340 |  it      | -1.970 | 13.95%\n",
      "|  4577 |  easier  | -2.133 | 11.84%\n",
      "|   284 |  to      | -0.470 | 62.47%\n",
      "Is this the joint probability across the whole vocab? 0.0112%\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([s_string], return_tensors=\"pt\")\n",
    "\n",
    "# Example 1: Print the scores for each token generated with Greedy Search\n",
    "outputs = model.generate(**inputs, max_new_tokens=5, top_k = 1, return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
    "# encoder-decoder models, like BART or T5.\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Iterate over the generated tokens and transition scores\n",
    "df = pd.DataFrame({'token': generated_tokens[0].numpy(), 'trans_scores': transition_scores[0].numpy()})\n",
    "df['token_str'] = df['token'].apply(lambda x: tokenizer.decode(x))\n",
    "df['trans_prob'] = df['trans_scores'].apply(lambda x: np.exp(x))\n",
    "df = df[['token', 'token_str', 'trans_scores', 'trans_prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$\n",
      "\\begin{tabular}{lrlrr}\n",
      "\\toprule\n",
      " & token & token_str & trans_scores & trans_prob \\\\\n",
      "\\midrule\n",
      "0 & 284 &  to & -1.336478 & 0.262770 \\\\\n",
      "1 & 787 &  make & -3.182834 & 0.041468 \\\\\n",
      "2 & 340 &  it & -1.969614 & 0.139511 \\\\\n",
      "3 & 4577 &  easier & -2.133453 & 0.118428 \\\\\n",
      "4 & 284 &  to & -0.470468 & 0.624710 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "#| output: 'asis'\n",
    "#| echo: false\n",
    "print(f\"\"\"$$\n",
    "{df.to_latex()}$$\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- e.g. \"\\n\\n\" and \" \\n\\n\" are different\n",
    "- Have to use legacy API to get the logs (risky?)\n",
    "- Seed parameter (almost) always the same output for the same settings, even more \"almost\" with temperature=0.0?\n",
    "- [\"We’re also launching a feature to return the log probabilities for the most likely output tokens generated by GPT-4 Turbo and GPT-3.5 Turbo in the next few weeks, which will be useful for building features such as autocomplete in a search experience.\"](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)\n",
    "- Also save the `system_fingerprint`, to keep track of the state of the model. If the model itself gets updated the same seed might yield different results.\n",
    "- Can set the seed, but not the fingerprint\n",
    "- Can run LLAMA2 locally, at least the smallest version?\n",
    "- With OpenAI API, we cannot access the first input layer directly, have to go through prompts\n",
    "- \"[inputs_embeds](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel) (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert input_ids indices into associated vectors than the model’s internal embedding lookup matrix.\"\n",
    "- For GPT-2 we can make the output deterministic\n",
    "- We can give in an input embedding instead of a tokenized phrase `model(inputs_embeds=embeds)`\n",
    "- [Thread on logit scores and their different variants](https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/13)\n",
    "  - Transition scores: \"transition_scores contains scores for the tokens that were selected at generation time. You can set normalize_logits=True to ensure they are normalized at a token level (i.e. to ensure the sum of probabilities for all vocabulary at a given generation step is 1).\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
