@inproceedings{benderDangersStochasticParrots2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? ðŸ¦œ},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  pages = {610--623},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3442188.3445922},
  urldate = {2023-06-26},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/I8MP3WNP/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf}
}

@article{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1810.04805},
  urldate = {2023-07-09},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,notion}
}

@incollection{eggersChoiceBasedConjointAnalysis2022,
  title = {Choice-{{Based Conjoint Analysis}}},
  booktitle = {Handbook of {{Market Research}}},
  author = {Eggers, Felix and Sattler, Henrik and Teichert, Thorsten and V{\"o}lckner, Franziska},
  editor = {Homburg, Christian and Klarmann, Martin and Vomberg, Arnd},
  year = {2022},
  pages = {781--819},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-57413-4_23},
  urldate = {2024-05-28},
  isbn = {978-3-319-57411-0 978-3-319-57413-4},
  langid = {english},
  file = {/Users/hoener/Zotero/storage/72IJB6VU/Eggers et al. - 2022 - Choice-Based Conjoint Analysis.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Publisher's description},
  isbn = {978-0-262-33737-3},
  langid = {english},
  keywords = {notion},
  annotation = {OCLC: 1183962587},
  file = {/Users/hoener/Downloads/goodfellowDeepLearning2016.pdf}
}

@misc{huggingfaceGenerateText,
  title = {How to Generate Text: Using Different Decoding Methods for Language Generation with {{Transformers}} --- Huggingface.Co}
}

@article{liFrontiersDeterminingValidity2024a,
  title = {Frontiers: {{Determining}} the {{Validity}} of {{Large Language Models}} for {{Automated Perceptual Analysis}}},
  shorttitle = {Frontiers},
  author = {Li, Peiyao and Castelo, Noah and Katona, Zsolt and Sarvary, Miklos},
  year = {2024},
  month = jan,
  journal = {Marketing Science},
  volume = {43},
  number = {2},
  pages = {254--266},
  publisher = {INFORMS},
  issn = {0732-2399},
  doi = {10.1287/mksc.2023.0454},
  urldate = {2024-05-13},
  abstract = {This paper explores the potential of large language models (LLMs) to substitute for human participants in market research. Such LLMs can be used to generate text given a prompt. We argue that perceptual analysis is a particularly promising use case for such automated market research for certain product categories. The proposed new method generates outputs that closely match those generated from human surveys: agreement rates between human- and LLM- generated data sets reach over 75\%. Moreover, this applies for perceptual analysis based on both brand similarity measures and product attribute ratings. The paper demonstrates that, for some categories, this new method of fully or partially automated market research will increase the efficiency of market research by meaningfully speeding up the process and potentially reducing the cost. Further results also suggest that with an ever larger training corpus applied to large language models, LLM-based market research will be applicable to answer more nuanced questions based on demographic variables or contextual variation that would be prohibitively expensive or infeasible with human respondents. History: Catherine Tucker served as the senior editor. This paper was accepted through the Marketing Science: Frontiers review process. Funding: This work was supported by the Social Sciences and Humanities Research Council of Canada [Grant 430-2021-00057]. Supplemental Material: The online appendix and data files are available at https://doi-org.vu-nl.idm.oclc.org/10.1287/mksc.2023.0454.},
  keywords = {artificial Intelligence,large language model,market research,natural language processing,notion,perceptual maps},
  file = {/Users/hoener/Zotero/storage/HHANZHZZ/Li et al. - 2024 - Frontiers Determining the Validity of Large Langu.pdf}
}

@techreport{ludwigMachineLearningTool2023,
  title = {Machine {{Learning}} as a {{Tool}} for {{Hypothesis Generation}}},
  author = {Ludwig, Jens and Mullainathan, Sendhil},
  year = {2023},
  month = mar,
  number = {w31017},
  pages = {w31017},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w31017},
  urldate = {2023-10-12},
  langid = {english},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/YHISPA8M/Ludwig and Mullainathan - 2023 - Machine Learning as a Tool for Hypothesis Generati.pdf}
}

@misc{MegaSynIntegratingGenerative,
  title = {{{MegaSyn}}: {{Integrating Generative Molecular Design}}, {{Automated Analog Designer}}, and {{Synthetic Viability Prediction}} {\textbar} {{ACS Omega}}},
  urldate = {2024-04-07},
  howpublished = {https://pubs.acs.org/doi/full/10.1021/acsomega.2c01404},
  keywords = {notion},
  file = {/Users/hoener/Zotero/storage/TNFMQTH7/acsomega.html}
}

@misc{metaMetaLlama,
  title = {Meta {{Llama}} 3 --- Llama.Meta.Com}
}

@article{radford2018improving,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year = {2018},
  publisher = {OpenAI},
  keywords = {notion}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1908.10084},
  urldate = {2023-07-09},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,notion}
}

@article{shenBaselineNeedsMore2018,
  title = {Baseline {{Needs More Love}}: {{On Simple Word-Embedding-Based Models}} and {{Associated Pooling Mechanisms}}},
  shorttitle = {Baseline {{Needs More Love}}},
  author = {Shen, Dinghan and Wang, Guoyin and Wang, Wenlin and Min, Martin Renqiang and Su, Qinliang and Zhang, Yizhe and Li, Chunyuan and Henao, Ricardo and Carin, Lawrence},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1805.09843},
  urldate = {2023-07-09},
  abstract = {Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring a substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging. The source code and datasets can be obtained from https:// github.com/dinghanshen/SWEM.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),notion}
}

@article{vaswani2017attention,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {notion}
}

@misc{wangDisentangledRepresentationLearning2024,
  title = {Disentangled {{Representation Learning}}},
  author = {Wang, Xin and Chen, Hong and Tang, Si'ao and Wu, Zihao and Zhu, Wenwu},
  year = {2024},
  month = may,
  number = {arXiv:2211.11695},
  eprint = {2211.11695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11695},
  urldate = {2024-05-29},
  abstract = {Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, and data mining. In this article, we comprehensively investigate DRL from various aspects including motivations, definitions, methodologies, evaluations, applications, and model designs. We first present two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition for disentangled representation learning. We further categorize the methodologies for DRL into four groups from the following perspectives, the model type, representation structure, supervision signal, and independence assumption. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hoener/Zotero/storage/BJKSADUV/Wang et al. - 2024 - Disentangled Representation Learning.pdf;/Users/hoener/Zotero/storage/J2NTYGXR/2211.html}
}

@misc{wangDisentangledRepresentationLearning2024a,
  title = {Disentangled {{Representation Learning}}},
  author = {Wang, Xin and Chen, Hong and Tang, Si'ao and Wu, Zihao and Zhu, Wenwu},
  year = {2024},
  month = may,
  number = {arXiv:2211.11695},
  eprint = {2211.11695},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-29},
  abstract = {Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, and data mining. In this article, we comprehensively investigate DRL from various aspects including motivations, definitions, methodologies, evaluations, applications, and model designs. We first present two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition for disentangled representation learning. We further categorize the methodologies for DRL into four groups from the following perspectives, the model type, representation structure, supervision signal, and independence assumption. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hoener/Zotero/storage/AI7GI9G5/Wang et al. - 2024 - Disentangled Representation Learning.pdf}
}
